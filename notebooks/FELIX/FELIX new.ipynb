{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC2TEsX5sPQj"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f1HBYYYFr7Rc"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers[torch] datasets pydantic==1.10 langchain[llms] openai tiktoken hdbscan wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t0DJiQH13cPD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_DISABLE_SYMLINK\"] = \"True\"\n",
    "\n",
    "# Set API keys as environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "os.environ[\"OPENAI_ORG\"] = \"...\"\n",
    "os.environ[\"WANDB_KEY\"] = \"05cd0a25e7a53f3adb89d0e4dfdfa499309b1dc9\"\n",
    "\n",
    "# Login to Weights & Biases for experiment tracking\n",
    "# wandb.login(key=os.environ[\"WANDB_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J-_LKY9v8Wrv"
   },
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_splIj31pfw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NI6M7TIzI3HS"
   },
   "outputs": [],
   "source": [
    "# Import HuggingFace datasets package\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define container class for datasets\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, id, short_name, X_train, X_test, y_train, y_test, pos_class, context_train, context_test, train_zero_shot=False):\n",
    "        self.id = id\n",
    "        self.short_name = short_name\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.pos_class = pos_class # Class that counts as positive (for measuring precision, recall, and F1 score)\n",
    "        self.context_train = context_train\n",
    "        self.context_test = context_test\n",
    "        self.train_zero_shot = train_zero_shot\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Dataset(\\n\" + f\"\\tid='{self.id}'\\n\" + f\"\\tshort_name='{self.short_name}'\\n\" + f\"\\tcontext_train='{self.context_train}'\\n\" + f\"\\tcontext_test='{self.context_test}'\\n\" + f\"\\tclasses={list(pd.concat([self.y_train, self.y_test]).unique())}\\n\" + f\"\\tpos_class='{self.pos_class}'\\n\" + f\"\\ttrain_zero_shot={self.train_zero_shot}\\n\" + f\"\\tX_train.shape={self.X_train.shape}\\n\" + f\"\\tX_test.shape={self.X_test.shape}\\n\" + f\"\\ty_train.shape={self.y_train.shape}\\n\" + f\"\\ty_test.shape={self.y_test.shape}\\n\" + \")\"\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hrhn5N1xKIje"
   },
   "outputs": [],
   "source": [
    "# Initialize list of datasets\n",
    "dataset_list = {}\n",
    "\n",
    "# Set overall sample size\n",
    "n_train = 100\n",
    "n_test = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEcYM8nv_80n",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fake Scientific Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fEtiVn248QLH",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='tum-nlp/IDMGSP'\n",
       "\tshort_name='papers'\n",
       "\tcontext_train='Examples are coming from a dataset with scientific papers that are either 'HUMAN-WRITTEN' (i.e., written by a real person) or 'MACHINE-GENERATED' (i.e., generated by a machine learning model). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HUMAN-WRITTEN' and 'MACHINE-GENERATED' scientific papers.'\n",
       "\tcontext_test='Examples are coming from a dataset with scientific papers that are either 'HUMAN-WRITTEN' (i.e., written by a real person) or 'MACHINE-GENERATED' (i.e., generated by a machine learning model). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HUMAN-WRITTEN' and 'MACHINE-GENERATED' scientific papers.'\n",
       "\tclasses=['HUMAN-WRITTEN', 'MACHINE-GENERATED']\n",
       "\tpos_class='MACHINE-GENERATED'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100, 4)\n",
       "\tX_test.shape=(100, 4)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the original training dataset\n",
    "train = datasets.load_dataset(\"tum-nlp/IDMGSP\", \"train+gpt3\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "train_shuffled = train.shuffle(seed=seed)\n",
    "\n",
    "# Separate the different sources/generators\n",
    "train_real = train_shuffled.filter(lambda x: x[\"src\"] == \"real\")\n",
    "train_scigen = train_shuffled.filter(lambda x: x[\"src\"] == \"scigen\")\n",
    "train_galactica = train_shuffled.filter(lambda x: x[\"src\"] == \"galactica\")\n",
    "train_gpt2 = train_shuffled.filter(lambda x: x[\"src\"] == \"gpt2\")\n",
    "train_gpt3 = train_shuffled.filter(lambda x: x[\"src\"] == \"gpt3\")\n",
    "train_chatgpt = train_shuffled.filter(lambda x: x[\"src\"] == \"chatgpt\")\n",
    "\n",
    "# Create one training dataset with alternating real and fake examples and equal number of examples from each fake source\n",
    "train_fake = datasets.interleave_datasets([train_scigen, train_galactica, train_gpt2, train_gpt3, train_chatgpt])\n",
    "data_train = datasets.interleave_datasets([train_real, train_fake])\n",
    "\n",
    "\n",
    "\n",
    "# Load the original test datasets\n",
    "test = datasets.load_dataset(\"tum-nlp/IDMGSP\", \"classifier_input\", split=\"test\")\n",
    "ood_gpt3 = datasets.load_dataset(\"tum-nlp/IDMGSP\", \"ood_gpt3\", split=\"test\")\n",
    "\n",
    "# Randomly shuffle the datasets\n",
    "test_shuffled = test.shuffle(seed=seed)\n",
    "test_gpt3 = ood_gpt3.shuffle(seed=seed)\n",
    "\n",
    "# Separate the different sources/generators\n",
    "test_real = test_shuffled.filter(lambda x: x[\"src\"] == \"real\")\n",
    "test_scigen = test_shuffled.filter(lambda x: x[\"src\"] == \"scigen\")\n",
    "test_galactica = test_shuffled.filter(lambda x: x[\"src\"] == \"galactica\")\n",
    "test_gpt2 = test_shuffled.filter(lambda x: x[\"src\"] == \"gpt2\")\n",
    "test_chatgpt = test_shuffled.filter(lambda x: x[\"src\"] == \"chatgpt\")\n",
    "\n",
    "# Create one test dataset with alternating real and fake examples and equal number of examples from each fake source\n",
    "test_fake = datasets.interleave_datasets([test_scigen, test_galactica, test_gpt2, test_gpt3, test_chatgpt])\n",
    "data_test = datasets.interleave_datasets([test_real, test_fake])\n",
    "\n",
    "\n",
    "\n",
    "# Package dataset in a Dataset container\n",
    "d = Dataset(\n",
    "    id=\"tum-nlp/IDMGSP\",\n",
    "    short_name=\"papers\",\n",
    "    context_train=\"Examples are coming from a dataset with scientific papers that are either 'HUMAN-WRITTEN' (i.e., written by a real person) or 'MACHINE-GENERATED' (i.e., generated by a machine learning model). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HUMAN-WRITTEN' and 'MACHINE-GENERATED' scientific papers.\",\n",
    "    context_test=\"Examples are coming from a dataset with scientific papers that are either 'HUMAN-WRITTEN' (i.e., written by a real person) or 'MACHINE-GENERATED' (i.e., generated by a machine learning model). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HUMAN-WRITTEN' and 'MACHINE-GENERATED' scientific papers.\",\n",
    "    pos_class=\"MACHINE-GENERATED\",\n",
    "    X_train=data_train.to_pandas().head(n_train)[[\"title\", \"abstract\", \"introduction\", \"conclusion\"]].rename(columns={\"title\": \"Title\", \"abstract\": \"Abstract\", \"introduction\": \"Introduction\", \"conclusion\": \"Conclusion\"}),\n",
    "    X_test=data_test.to_pandas().head(n_test)[[\"title\", \"abstract\", \"introduction\", \"conclusion\"]].rename(columns={\"title\": \"Title\", \"abstract\": \"Abstract\", \"introduction\": \"Introduction\", \"conclusion\": \"Conclusion\"}),\n",
    "    y_train=data_train.to_pandas().head(n_train)[\"label\"].map({0: \"HUMAN-WRITTEN\", 1: \"MACHINE-GENERATED\"}),\n",
    "    y_test=data_test.to_pandas().head(n_test)[\"label\"].map({0: \"HUMAN-WRITTEN\", 1: \"MACHINE-GENERATED\"})\n",
    ")\n",
    "\n",
    "# Add to dataset list\n",
    "dataset_list[d.short_name] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1sxQiWHErgs",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EfvlYYgwEnyK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='GonzaloA/fake_news'\n",
       "\tshort_name='fake-news-2'\n",
       "\tcontext_train='Examples are coming from a dataset with news articles that are either 'FAKE NEWS' or 'REAL NEWS'. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'FAKE NEWS' and 'REAL NEWS' articles.'\n",
       "\tcontext_test='Examples are coming from a dataset with news articles that are either 'FAKE NEWS' or 'REAL NEWS'. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'FAKE NEWS' and 'REAL NEWS' articles.'\n",
       "\tclasses=['FAKE NEWS', 'REAL NEWS']\n",
       "\tpos_class='FAKE NEWS'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100, 2)\n",
       "\tX_test.shape=(100, 2)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "data_train = datasets.load_dataset(\"GonzaloA/fake_news\", split=\"train\")\n",
    "\n",
    "# Randomly shuffle the datasets\n",
    "train_shuffled = data_train.shuffle(seed=seed)\n",
    "\n",
    "# Separate the different classes (fake news and real news)\n",
    "train_fake = train_shuffled.filter(lambda x: x[\"label\"] == 0)\n",
    "train_real = train_shuffled.filter(lambda x: x[\"label\"] == 1)\n",
    "\n",
    "# Create one dataset with alternating fake and real news examples\n",
    "data_train = datasets.interleave_datasets([train_fake, train_real], stopping_strategy=\"first_exhausted\")\n",
    "\n",
    "# Convert to a Pandas DataFrame, split data and label columns, and limit to the training set size\n",
    "df_train = data_train.to_pandas()\n",
    "df_train = df_train.head(n_train)\n",
    "df_train = df_train.rename(columns={\"title\": \"Title\", \"text\": \"Text\"})\n",
    "df_train[\"label\"] = df_train.apply(lambda row: \"FAKE NEWS\" if row[\"label\"] == 0 else \"REAL NEWS\", axis=1)\n",
    "X_train = df_train[[\"Title\", \"Text\"]]\n",
    "y_train = df_train[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the test dataset\n",
    "data_test = datasets.load_dataset(\"GonzaloA/fake_news\", split=\"test\")\n",
    "\n",
    "# Randomly shuffle the datasets\n",
    "test_shuffled = data_test.shuffle(seed=seed)\n",
    "\n",
    "# Separate the different classes (fake news and real news)\n",
    "test_fake = test_shuffled.filter(lambda x: x[\"label\"] == 0)\n",
    "test_real = test_shuffled.filter(lambda x: x[\"label\"] == 1)\n",
    "\n",
    "# Create one dataset with alternating fake and real news examples\n",
    "data_test = datasets.interleave_datasets([test_fake, test_real], stopping_strategy=\"first_exhausted\")\n",
    "\n",
    "# Convert to a Pandas DataFrame, split data and label columns, and limit to the test set size\n",
    "df_test = data_test.to_pandas()\n",
    "df_test = df_test.head(n_test)\n",
    "df_test = df_test.rename(columns={\"title\": \"Title\", \"text\": \"Text\"})\n",
    "df_test[\"label\"] = df_test.apply(lambda row: \"FAKE NEWS\" if row[\"label\"] == 0 else \"REAL NEWS\", axis=1)\n",
    "X_test = df_test[[\"Title\", \"Text\"]]\n",
    "y_test = df_test[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "# Package dataset in a Dataset container\n",
    "d = Dataset(\n",
    "    id=\"GonzaloA/fake_news\",\n",
    "    short_name=\"fake-news-2\",\n",
    "    context_train=\"Examples are coming from a dataset with news articles that are either 'FAKE NEWS' or 'REAL NEWS'. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'FAKE NEWS' and 'REAL NEWS' articles.\",\n",
    "    context_test=\"Examples are coming from a dataset with news articles that are either 'FAKE NEWS' or 'REAL NEWS'. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'FAKE NEWS' and 'REAL NEWS' articles.\",\n",
    "    pos_class=\"FAKE NEWS\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Add to dataset list\n",
    "dataset_list[d.short_name] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unSRgROhYfkL",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hatespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-oxxMVu9YhdH",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='hate_speech18'\n",
       "\tshort_name='hate-speech'\n",
       "\tcontext_train='Examples are posts sampled from a white supremacist forum and are either 'HATE SPEECH' (when the posts contain hate speech) or 'NO HATE SPEECH' (when the posts do not contain hate speech). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HATE SPEECH' and 'NO HATE SPEECH' forum posts.'\n",
       "\tcontext_test='Examples are posts sampled from a white supremacist forum and are either 'HATE SPEECH' (when the posts contain hate speech) or 'NO HATE SPEECH' (when the posts do not contain hate speech). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HATE SPEECH' and 'NO HATE SPEECH' forum posts.'\n",
       "\tclasses=['NO HATE SPEECH', 'HATE SPEECH']\n",
       "\tpos_class='HATE SPEECH'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100,)\n",
       "\tX_test.shape=(100,)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = datasets.load_dataset(\"hate_speech18\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Remove samples that rely on the context of previous messages\n",
    "data = data.filter(lambda x: x[\"num_contexts\"] == 0)\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "data_shuffled = data.shuffle(seed=seed)\n",
    "\n",
    "# Separate the different classes (hate speech and no hate speech)\n",
    "data_hate = data_shuffled.filter(lambda x: x[\"label\"] == 1)\n",
    "data_no_hate = data_shuffled.filter(lambda x: x[\"label\"] == 0)\n",
    "\n",
    "# Create one dataset with alternating hate and no hate examples\n",
    "data = datasets.interleave_datasets([data_no_hate, data_hate], stopping_strategy=\"first_exhausted\")\n",
    "\n",
    "# Convert to a pandas dataframe\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Map the class numbers to natural-language class labels\n",
    "df[\"label\"] = df.apply(lambda row: \"HATE SPEECH\" if row[\"label\"] == 1 else \"NO HATE SPEECH\", axis=1)\n",
    "\n",
    "# Split data and label\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split train and test sets\n",
    "X_train = X.iloc[0:n_train]\n",
    "y_train = y.iloc[0:n_train]\n",
    "X_test = X.iloc[n_train:(n_train+n_test)]\n",
    "y_test = y.iloc[n_train:(n_train+n_test)]\n",
    "\n",
    "# Package dataset in a Dataset container\n",
    "d = Dataset(\n",
    "    id=\"hate_speech18\",\n",
    "    short_name=\"hate-speech\",\n",
    "    context_train=\"Examples are posts sampled from a white supremacist forum and are either 'HATE SPEECH' (when the posts contain hate speech) or 'NO HATE SPEECH' (when the posts do not contain hate speech). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HATE SPEECH' and 'NO HATE SPEECH' forum posts.\",\n",
    "    context_test=\"Examples are posts sampled from a white supremacist forum and are either 'HATE SPEECH' (when the posts contain hate speech) or 'NO HATE SPEECH' (when the posts do not contain hate speech). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'HATE SPEECH' and 'NO HATE SPEECH' forum posts.\",\n",
    "    pos_class=\"HATE SPEECH\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Add to dataset list\n",
    "dataset_list[d.short_name] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4WeCc7vxzdP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reviews Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J3iU4Jeax36i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training dataset ...\n",
      "Preparing training dataset ...\n",
      "Downloading test dataset ...\n",
      "Preparing test dataset ...\n",
      "Packaging the dataset ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='amazon_polarity'\n",
       "\tshort_name='reviews-amazon'\n",
       "\tcontext_train='Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tcontext_test='Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tclasses=['POSITIVE', 'NEGATIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100, 2)\n",
       "\tX_test.shape=(100, 2)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the training dataset\n",
    "print(\"Downloading training dataset ...\")\n",
    "data_train = datasets.load_dataset(\"amazon_polarity\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "print(\"Preparing training dataset ...\")\n",
    "train_shuffled = data_train.shuffle(seed=seed)\n",
    "\n",
    "# Separate positive and negative reviews\n",
    "train_positive = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 1)\n",
    "train_negative = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 0)\n",
    "\n",
    "# Create one training dataset with alternating examples from both classes\n",
    "df_train = datasets.interleave_datasets([train_positive, train_negative], stopping_strategy=\"first_exhausted\").select(range(n_train)).to_pandas()\n",
    "\n",
    "# Format the dataset\n",
    "X_train = df_train.rename(columns={\"title\": \"Title\", \"content\": \"Content\"})[[\"Title\", \"Content\"]]\n",
    "y_train = df_train[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n",
    "\n",
    "\n",
    "\n",
    "# Download the test dataset\n",
    "print(\"Downloading test dataset ...\")\n",
    "data_test = datasets.load_dataset(\"amazon_polarity\", split=\"test\")\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "print(\"Preparing test dataset ...\")\n",
    "test_shuffled = data_test.shuffle(seed=seed)\n",
    "\n",
    "# Separate positive and negative reviews\n",
    "test_positive = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 1)\n",
    "test_negative = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 0)\n",
    "\n",
    "# Create one test dataset with alternating examples from both classes\n",
    "df_test = datasets.interleave_datasets([test_positive, test_negative], stopping_strategy=\"first_exhausted\").select(range(n_test)).to_pandas()\n",
    "\n",
    "# Format the dataset\n",
    "X_test = df_test.rename(columns={\"title\": \"Title\", \"content\": \"Content\"})[[\"Title\", \"Content\"]]\n",
    "y_test = df_test[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n",
    "\n",
    "\n",
    "\n",
    "# Package dataset in a Dataset container\n",
    "print(\"Packaging the dataset ...\")\n",
    "d = Dataset(\n",
    "    id=\"amazon_polarity\",\n",
    "    short_name=\"reviews-amazon\",\n",
    "    context_train=\"Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n",
    "    context_test=\"Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n",
    "    pos_class=\"POSITIVE\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Add to dataset list\n",
    "dataset_list[d.short_name] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcQTEKt0QK1p",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reviews Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hh4L_ntgQM2m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training dataset ...\n",
      "Preparing training dataset ...\n",
      "Downloading test dataset ...\n",
      "Preparing test dataset ...\n",
      "Packaging the dataset ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='yelp_polarity'\n",
       "\tshort_name='reviews-yelp'\n",
       "\tcontext_train='Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tcontext_test='Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tclasses=['POSITIVE', 'NEGATIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100, 1)\n",
       "\tX_test.shape=(100, 1)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the training dataset\n",
    "print(\"Downloading training dataset ...\")\n",
    "data_train = datasets.load_dataset(\"yelp_polarity\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "print(\"Preparing training dataset ...\")\n",
    "train_shuffled = data_train.shuffle(seed=seed)\n",
    "\n",
    "# Separate positive and negative reviews\n",
    "train_positive = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 1)\n",
    "train_negative = train_shuffled.select(range(n_train*100)).filter(lambda x: x[\"label\"] == 0)\n",
    "\n",
    "# Create one training dataset with alternating examples from both classes\n",
    "df_train = datasets.interleave_datasets([train_positive, train_negative], stopping_strategy=\"first_exhausted\").select(range(n_train)).to_pandas()\n",
    "\n",
    "# Format the dataset\n",
    "X_train = df_train.rename(columns={\"text\": \"Text\"})[[\"Text\"]]\n",
    "y_train = df_train[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n",
    "\n",
    "\n",
    "\n",
    "# Download the test dataset\n",
    "print(\"Downloading test dataset ...\")\n",
    "data_test = datasets.load_dataset(\"yelp_polarity\", split=\"test\")\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "print(\"Preparing test dataset ...\")\n",
    "test_shuffled = data_test.shuffle(seed=seed)\n",
    "\n",
    "# Separate positive and negative reviews\n",
    "test_positive = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 1)\n",
    "test_negative = test_shuffled.select(range(n_test*100)).filter(lambda x: x[\"label\"] == 0)\n",
    "\n",
    "# Create one test dataset with alternating examples from both classes\n",
    "df_test = datasets.interleave_datasets([test_positive, test_negative], stopping_strategy=\"first_exhausted\").select(range(n_test)).to_pandas()\n",
    "\n",
    "# Format the dataset\n",
    "X_test = df_test.rename(columns={\"text\": \"Text\"})[[\"Text\"]]\n",
    "y_test = df_test[\"label\"].map({0: \"NEGATIVE\", 1: \"POSITIVE\"})\n",
    "\n",
    "\n",
    "\n",
    "# Package dataset in a Dataset container\n",
    "print(\"Packaging the dataset ...\")\n",
    "d = Dataset(\n",
    "    id=\"yelp_polarity\",\n",
    "    short_name=\"reviews-yelp\",\n",
    "    context_train=\"Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n",
    "    context_test=\"Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.\",\n",
    "    pos_class=\"POSITIVE\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Add to dataset list\n",
    "dataset_list[d.short_name] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln7LyKru6udZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mW9jy8Yd6w9H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='cardiffnlp/tweet_sentiment_multilingual'\n",
       "\tshort_name='sentiment'\n",
       "\tcontext_train='Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.'\n",
       "\tcontext_test='Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.'\n",
       "\tclasses=['NEGATIVE', 'POSITIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100,)\n",
       "\tX_test.shape=(100,)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download and combine training datasets in different languages\n",
    "train_english = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_arabic = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"arabic\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_french = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_german = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_hindi = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_italian = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_portuguese = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "train_spanish = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"train\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "df_train = datasets.interleave_datasets([train_english, train_arabic, train_french, train_german, train_hindi, train_italian, train_portuguese, train_spanish], stopping_strategy=\"first_exhausted\").select(range(n_train)).to_pandas()\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "df_train = df_train.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# Format the dataset\n",
    "X_train = df_train[\"text\"]\n",
    "y_train = df_train[\"label\"].map({0: \"NEGATIVE\", 2: \"POSITIVE\"})\n",
    "\n",
    "\n",
    "\n",
    "# Download and combine test datasets in different languages\n",
    "test_english = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"english\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_arabic = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"arabic\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_french = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"french\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_german = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"german\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_hindi = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"hindi\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_italian = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"italian\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_portuguese = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"portuguese\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "test_spanish = datasets.load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split=\"test\").filter(lambda x: x[\"label\"] in [0, 2])\n",
    "df_test = datasets.interleave_datasets([test_english, test_arabic, test_french, test_german, test_hindi, test_italian, test_portuguese, test_spanish], stopping_strategy=\"first_exhausted\").select(range(n_test)).to_pandas()\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "df_test = df_test.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# Format the dataset\n",
    "X_test = df_test[\"text\"]\n",
    "y_test = df_test[\"label\"].map({0: \"NEGATIVE\", 2: \"POSITIVE\"})\n",
    "\n",
    "\n",
    "\n",
    "# Package dataset in a Dataset container\n",
    "d = Dataset(\n",
    "    id=\"cardiffnlp/tweet_sentiment_multilingual\",\n",
    "    short_name=\"sentiment\",\n",
    "    context_train=\"Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.\",\n",
    "    context_test=\"Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.\",\n",
    "    pos_class=\"POSITIVE\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Add to dataset list\n",
    "dataset_list[d.short_name] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve_kudeg3Yvm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kx3M_DLE-tw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pMxn07AHs40z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# class PromptingTQDM(tqdm):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}{postfix}]\", *args, **kwargs)\n",
    "#         self.reset_stats()\n",
    "\n",
    "#     def reset_stats(self):\n",
    "#         self.tokens_in = 0\n",
    "#         self.tokens_out = 0\n",
    "#         self.costs = 0\n",
    "#         self.n_features = 0\n",
    "#         self.n_iter = 0\n",
    "\n",
    "#     def log_stats(self, tokens_in, tokens_out, cost, n_features):\n",
    "#         self.tokens_in += tokens_in\n",
    "#         self.tokens_out += tokens_out\n",
    "#         self.costs += cost\n",
    "#         self.n_features += n_features\n",
    "#         self.n_iter += 1\n",
    "\n",
    "#         avg_tokens_in = self.tokens_in / self.n_iter\n",
    "#         avg_tokens_out = self.tokens_out / self.n_iter\n",
    "#         avg_cost = self.costs / self.n_iter\n",
    "\n",
    "#         self.set_postfix_str(f\"in {avg_tokens_in:.0f} tokens/{self.unit}, out {avg_tokens_out:.0f} tokens/{self.unit}, {avg_cost:.5f} USD/{self.unit}, {self.costs:.2f} USD total, {self.n_features:.0f} features total\", refresh=True)\n",
    "\n",
    "\n",
    "class NumericalFeature(BaseModel):\n",
    "    name: str = Field(description=\"concise name of the feature\")\n",
    "    zero: str = Field(description=\"meaning of feature value of 0\")\n",
    "    ten: str = Field(description=\"meaning of feature value of 10\")\n",
    "    description: str = Field(description=\"short description of the meaning of this feature\")\n",
    "\n",
    "\n",
    "class NumericalFeatureSet(BaseModel):\n",
    "    features: List[NumericalFeature] = Field(description=\"list of numeric features\")\n",
    "\n",
    "\n",
    "class CategoricalFeature(BaseModel):\n",
    "    name: str = Field(description=\"concise name of the feature\")\n",
    "    possible_values: List[str] = Field(description=\"list of 2-5 different possible values allowed for this feature; the feature can take exactly one of these values at once\")\n",
    "    description: str = Field(description=\"short description of the meaning of this feature\")\n",
    "\n",
    "\n",
    "class CategoricalFeatureSet(BaseModel):\n",
    "    features: List[CategoricalFeature] = Field(description=\"list of categorical features\")\n",
    "\n",
    "\n",
    "# A generic callback class that can be defined for FELIX and is called for every LLM request. Allows tracking of prompts, costs, and token consumption\n",
    "class FELIXCallback:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def features_generated_for_pair(self, llm, example_a: str, example_b: str, label_a: str, label_b: str, system_message: str, prompt_message: str, llm_output: str, features: NumericalFeatureSet | CategoricalFeatureSet, total_cost: float, total_tokens: int, prompt_tokens: int, completion_tokens: int):\n",
    "        pass\n",
    "\n",
    "    def example_transformed(self, llm, example: str, feature_set: NumericalFeatureSet | CategoricalFeatureSet, system_message: str, prompt_message: str, llm_output: str, scores, total_cost: float, total_tokens: int, prompt_tokens: int, completion_tokens: int):\n",
    "        pass\n",
    "\n",
    "    def error_encountered(self, llm, system_message, prompt_message, llm_output, error):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from hdbscan import HDBSCAN\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Вспомогательный класс для красивого отображения прогресса и логгирования\n",
    "# =============================================================================\n",
    "class PromptingTQDM(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            bar_format=(\n",
    "                \"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} \"\n",
    "                \"[{elapsed}<{remaining}, {rate_inv_fmt}{postfix}]\"\n",
    "            ),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.reset_stats()\n",
    "\n",
    "    def reset_stats(self):\n",
    "        # Начальные статистики\n",
    "        self.tokens_in = 0\n",
    "        self.tokens_out = 0\n",
    "        self.costs = 0\n",
    "        self.n_features = 0\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def log_stats(self, tokens_in, tokens_out, cost, n_features):\n",
    "        # Обновляем статистики\n",
    "        self.tokens_in += tokens_in\n",
    "        self.tokens_out += tokens_out\n",
    "        self.costs += cost\n",
    "        self.n_features += n_features\n",
    "        self.n_iter += 1\n",
    "\n",
    "        # Средние значения за все итерации\n",
    "        avg_tokens_in = self.tokens_in / self.n_iter\n",
    "        avg_tokens_out = self.tokens_out / self.n_iter\n",
    "        avg_cost = self.costs / self.n_iter\n",
    "\n",
    "        # Вывод в интерфейсе tqdm\n",
    "        self.set_postfix_str(\n",
    "            f\"in {avg_tokens_in:.0f} tokens/{self.unit}, \"\n",
    "            f\"out {avg_tokens_out:.0f} tokens/{self.unit}, \"\n",
    "            f\"{avg_cost:.5f} USD/{self.unit}, \"\n",
    "            f\"{self.costs:.2f} USD total, \"\n",
    "            f\"{self.n_features:.0f} features total\",\n",
    "            refresh=True\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Класс FELIX, адаптированный под работу с локальной моделью Ollama/Qwen\n",
    "# =============================================================================\n",
    "class FELIX(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context=None,\n",
    "        llm_name=\"qwen2.5:14b\",\n",
    "        llm_generation=None,\n",
    "        llm_scoring=None,\n",
    "        llm_embeddings=None,\n",
    "        temperature_generation=0.7,\n",
    "        temperature_scoring=0.0,\n",
    "        discrete_features=True,\n",
    "        zero_shot=False,\n",
    "        reschuffle_features=False,\n",
    "        keep_outlier_features=False,\n",
    "        callback=None,\n",
    "        verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "        ----------\n",
    "        context: Доп. контекст, который можно указывать при генерации фич\n",
    "        llm_name: Название/идентификатор модели Ollama (Qwen).\n",
    "        llm_generation: модель, которая используется для генерации фич (если не указана, используется llm_name).\n",
    "        llm_scoring: модель для скоринга (если не указана, используется llm_name).\n",
    "        llm_embeddings: модель/метод для вычисления эмбеддингов (можно использовать локальные эмбеддинги, если есть).\n",
    "        temperature_generation: температура при генерации фич.\n",
    "        temperature_scoring: температура при скоринге.\n",
    "        discrete_features: True, если фичи категориальные, False — числовые (0..10).\n",
    "        zero_shot: если True, используем \"zero-shot\" промпты.\n",
    "        reschuffle_features: если True, тасуем фичи после каждого scored example.\n",
    "        keep_outlier_features: если True, шумовые фичи (по кластеризации) не отбрасываются, а остаются в отдельных кластерах.\n",
    "        callback: объект FELIXCallback (необязательно).\n",
    "        verbose: включение детального вывода (progress-bar и т. д.).\n",
    "        \"\"\"\n",
    "        self.context = context\n",
    "\n",
    "        self.llm_name = llm_name\n",
    "        self.llm_generation = llm_generation if llm_generation else llm_name\n",
    "        self.llm_scoring = llm_scoring if llm_scoring else llm_name\n",
    "        self.llm_embeddings = llm_embeddings\n",
    "\n",
    "        self.temperature_generation = temperature_generation\n",
    "        self.temperature_scoring = temperature_scoring\n",
    "\n",
    "        self.discrete_features = discrete_features\n",
    "        self.zero_shot = zero_shot\n",
    "        self.reschuffle_features = reschuffle_features\n",
    "        self.keep_noise = keep_outlier_features\n",
    "\n",
    "        self.callback = callback\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Внутренние атрибуты\n",
    "        self._features = None\n",
    "        self._full_feature_set = None\n",
    "        self._pairs = None\n",
    "        self._pairwise_feature_sets = None\n",
    "        self._feature_embeddings = None\n",
    "        self._cluster_labels = None\n",
    "        self._global_duplicate_counter = 0\n",
    "        self._hdbscan = None\n",
    "\n",
    "        # Проверки базовых типов\n",
    "        self._validate_class_variables(\n",
    "            llm_name=True, \n",
    "            context=True,\n",
    "            temperature_generation=True,\n",
    "            temperature_scoring=True,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    # =========================================================================\n",
    "    # Методы сохранения/загрузки\n",
    "    # =========================================================================\n",
    "    def save_instance(self, filename):\n",
    "        if self.callback:\n",
    "            print(\"Warning: Parameter 'callback' is set but cannot be saved to instance.\")\n",
    "        if self._hdbscan:\n",
    "            print(\"Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\")\n",
    "\n",
    "        data = {\n",
    "            \"context\": self.context,\n",
    "            \"llm_name\": self.llm_name,\n",
    "            \"llm_generation\": self.llm_generation,\n",
    "            \"llm_scoring\": self.llm_scoring,\n",
    "            \"llm_embeddings\": self.llm_embeddings,\n",
    "            \"temperature_generation\": self.temperature_generation,\n",
    "            \"temperature_scoring\": self.temperature_scoring,\n",
    "            \"discrete_features\": self.discrete_features,\n",
    "            \"zero_shot\": self.zero_shot,\n",
    "            \"reschuffle_features\": self.reschuffle_features,\n",
    "            \"keep_noise\": self.keep_noise,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"full_feature_set\": self._full_feature_set.json() if self._full_feature_set else None,\n",
    "            \"features\": self._features.json() if self._features else None,\n",
    "            \"feature_embeddings\": self._feature_embeddings.tolist() if self._feature_embeddings is not None else None,\n",
    "            \"cluster_labels\": self._cluster_labels.tolist() if self._cluster_labels is not None else None,\n",
    "            \"duplicate_counter\": self._global_duplicate_counter\n",
    "        }\n",
    "\n",
    "        json_string = json.dumps(data, indent=4)\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(json_string)\n",
    "\n",
    "    def load_instance(self, filename):\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.context = data.get(\"context\")\n",
    "        self.llm_name = data.get(\"llm_name\")\n",
    "        self.llm_generation = data.get(\"llm_generation\")\n",
    "        self.llm_scoring = data.get(\"llm_scoring\")\n",
    "        self.llm_embeddings = data.get(\"llm_embeddings\")\n",
    "        self.temperature_generation = data.get(\"temperature_generation\")\n",
    "        self.temperature_scoring = data.get(\"temperature_scoring\")\n",
    "        self.discrete_features = data.get(\"discrete_features\")\n",
    "        self.zero_shot = data.get(\"zero_shot\", False)\n",
    "        self.reschuffle_features = data.get(\"reschuffle_features\", False)\n",
    "        self.keep_noise = data.get(\"keep_noise\", True)\n",
    "        self.verbose = data.get(\"verbose\", False)\n",
    "\n",
    "        ffs_json = data.get(\"full_feature_set\")\n",
    "        fs_json = data.get(\"features\")\n",
    "        if ffs_json:\n",
    "            if self.discrete_features:\n",
    "                self._full_feature_set = CategoricalFeatureSet.parse_raw(ffs_json)\n",
    "            else:\n",
    "                self._full_feature_set = NumericalFeatureSet.parse_raw(ffs_json)\n",
    "\n",
    "        if fs_json:\n",
    "            if self.discrete_features:\n",
    "                self._features = CategoricalFeatureSet.parse_raw(fs_json)\n",
    "            else:\n",
    "                self._features = NumericalFeatureSet.parse_raw(fs_json)\n",
    "\n",
    "        fe = data.get(\"feature_embeddings\")\n",
    "        if fe is not None:\n",
    "            self._feature_embeddings = np.array(fe)\n",
    "        cl = data.get(\"cluster_labels\")\n",
    "        if cl is not None:\n",
    "            self._cluster_labels = np.array(cl)\n",
    "\n",
    "        self._global_duplicate_counter = data.get(\"duplicate_counter\", 0)\n",
    "\n",
    "        # Восстановить callback / _hdbscan невозможно\n",
    "        self.callback = None\n",
    "        self._hdbscan = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # Методы fit/transform\n",
    "    # =========================================================================\n",
    "    def fit(self, X, y=None):\n",
    "        X, y = self._validate_data(X, y)\n",
    "        self._full_feature_set = self.generate_features(X, y)\n",
    "        self._features = self.consolidate_features(self._full_feature_set)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X, _ = self._validate_data(X, y, check_y=False)\n",
    "\n",
    "        llm_scoring = self._initialize_llm(self.llm_scoring, self.temperature_scoring)\n",
    "        X_transformed = []\n",
    "        with PromptingTQDM(total=len(X), desc=\"Scoring examples\", disable=not self.verbose) as progress_bar:\n",
    "            for i, example in X.items():\n",
    "                scores, _ = self._transform_example(llm_scoring, example, self._features)\n",
    "                X_transformed.append(scores)\n",
    "                if self.reschuffle_features:\n",
    "                    random.shuffle(self._features.features)\n",
    "                if self.verbose:\n",
    "                    progress_bar.update(1)\n",
    "        return pd.DataFrame(X_transformed)\n",
    "\n",
    "    def generate_features(self, X, y):\n",
    "        X, y = self._validate_data(X, y)\n",
    "        llm_generation = self._initialize_llm(self.llm_generation, self.temperature_generation)\n",
    "        self._pairs = self._generate_pairs(X, y)\n",
    "        self._pairwise_feature_sets = []\n",
    "\n",
    "        with PromptingTQDM(total=len(self._pairs), desc=\"Generating features\", disable=not self.verbose) as pbar:\n",
    "            for example_a, example_b, label_a, label_b in self._pairs:\n",
    "                feature_set, _ = self._generate_features_for_pair(\n",
    "                    llm_generation, example_a, example_b, label_a, label_b\n",
    "                )\n",
    "                self._pairwise_feature_sets.append(feature_set)\n",
    "                if self.verbose:\n",
    "                    pbar.update(1)\n",
    "\n",
    "        # Склеиваем всё в единый список\n",
    "        flat_list = []\n",
    "        for fs in self._pairwise_feature_sets:\n",
    "            flat_list += fs.features\n",
    "\n",
    "        if self.discrete_features:\n",
    "            full_feature_set = CategoricalFeatureSet(features=flat_list)\n",
    "        else:\n",
    "            full_feature_set = NumericalFeatureSet(features=flat_list)\n",
    "\n",
    "        return full_feature_set\n",
    "\n",
    "    def consolidate_features(self, feature_set):\n",
    "        self._feature_embeddings = self._create_feature_embeddings(feature_set)\n",
    "        self._cluster_labels = self._cluster_features(self._feature_embeddings)\n",
    "\n",
    "        if self.keep_noise:\n",
    "            # Шуму (label=-1) назначаем отдельные кластеры\n",
    "            if len(self._cluster_labels) > 0:\n",
    "                next_label = np.max(self._cluster_labels) + 1\n",
    "                for i in range(len(self._cluster_labels)):\n",
    "                    if self._cluster_labels[i] == -1:\n",
    "                        self._cluster_labels[i] = next_label\n",
    "                        next_label += 1\n",
    "        else:\n",
    "            # Удаляем шум, если он есть\n",
    "            if (self._cluster_labels == -1).all() and len(self._cluster_labels) > 0:\n",
    "                self._cluster_labels = np.zeros(len(self._cluster_labels), dtype=int)\n",
    "                if self.verbose:\n",
    "                    print(\"All features identified as noise. Treating them as one cluster\")\n",
    "\n",
    "            feature_set = feature_set.copy()\n",
    "            filtered_features = []\n",
    "            filtered_embeddings = []\n",
    "            filtered_labels = []\n",
    "            for f_item, emb, lbl in zip(feature_set.features, self._feature_embeddings, self._cluster_labels):\n",
    "                if lbl != -1:\n",
    "                    filtered_features.append(f_item)\n",
    "                    filtered_embeddings.append(emb)\n",
    "                    filtered_labels.append(lbl)\n",
    "            feature_set.features = filtered_features\n",
    "            self._feature_embeddings = np.array(filtered_embeddings)\n",
    "            self._cluster_labels = np.array(filtered_labels)\n",
    "\n",
    "        self._features = self._select_representative_features(feature_set, self._feature_embeddings, self._cluster_labels)\n",
    "        self._features = self._ensure_unique_feature_names(self._features)\n",
    "\n",
    "        print(f\"Consolidated to {len(self._features.features)} features ({'incl.' if self.keep_noise else 'excl.'} noise)\")\n",
    "        return self._features\n",
    "\n",
    "    def get_features_as_dataframe(self):\n",
    "        if not self._features:\n",
    "            raise ValueError(\"No features have been learned yet. Call fit() to learn a set of features.\")\n",
    "        if self.discrete_features:\n",
    "            data_dict = json.loads(self._features.json())[\"features\"]\n",
    "        else:\n",
    "            data_dict = json.loads(self._features.json())[\"features\"]\n",
    "        df = pd.DataFrame(data_dict)\n",
    "        df.columns = df.columns.str.replace(\"_\", \" \").str.title()\n",
    "        return df\n",
    "\n",
    "    # =========================================================================\n",
    "    # Вспомогательные методы\n",
    "    # =========================================================================\n",
    "    def _validate_class_variables(\n",
    "        self,\n",
    "        llm_name=False,\n",
    "        context=False,\n",
    "        temperature_generation=False,\n",
    "        temperature_scoring=False,\n",
    "        verbose=False\n",
    "    ):\n",
    "        if temperature_generation:\n",
    "            if not isinstance(self.temperature_generation, float):\n",
    "                raise ValueError(\"temperature_generation must be float.\")\n",
    "            if not 0.0 <= self.temperature_generation <= 1.0:\n",
    "                raise ValueError(\"temperature_generation must be in [0,1].\")\n",
    "        if temperature_scoring:\n",
    "            if not isinstance(self.temperature_scoring, float):\n",
    "                raise ValueError(\"temperature_scoring must be float.\")\n",
    "            if not 0.0 <= self.temperature_scoring <= 1.0:\n",
    "                raise ValueError(\"temperature_scoring must be in [0,1].\")\n",
    "        if verbose:\n",
    "            if not isinstance(self.verbose, bool):\n",
    "                raise ValueError(f\"verbose must be bool, got {type(self.verbose)}.\")\n",
    "        return True\n",
    "\n",
    "    def _validate_data(self, X, y, check_X=True, check_y=True):\n",
    "        if check_X:\n",
    "            if not isinstance(X, (list, np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                raise ValueError(\"X must be array-like.\")\n",
    "            if isinstance(X, (list, np.ndarray)):\n",
    "                if len(np.shape(X)) > 2:\n",
    "                    raise ValueError(\"X must have 1 or 2 dimensions.\")\n",
    "            elif isinstance(X, (pd.Series, pd.DataFrame)):\n",
    "                if len(X.shape) > 2:\n",
    "                    raise ValueError(\"X must have 1 or 2 dimensions.\")\n",
    "            if len(X) == 0:\n",
    "                raise ValueError(\"X cannot be empty.\")\n",
    "\n",
    "            # Превращаем X в DataFrame и сериализуем\n",
    "            X = pd.DataFrame(X)\n",
    "            X = self._serialize_dataframe(X)\n",
    "\n",
    "        if check_y:\n",
    "            if not isinstance(y, (list, np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                raise ValueError(\"y must be array-like.\")\n",
    "            if isinstance(y, (list, np.ndarray)):\n",
    "                if len(np.shape(y)) != 1:\n",
    "                    raise ValueError(\"y can only have 1 dimension.\")\n",
    "            elif isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "                if len(y.shape) != 1:\n",
    "                    raise ValueError(\"y can only have 1 dimension.\")\n",
    "            if len(y) == 0:\n",
    "                raise ValueError(\"y cannot be empty.\")\n",
    "            y = pd.Series(y)\n",
    "            if y.nunique() < 2:\n",
    "                raise ValueError(\"y must contain at least two unique values.\")\n",
    "            if check_X and len(X) != len(y):\n",
    "                raise ValueError(\"X and y must have the same length.\")\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _serialize_dataframe(self, df):\n",
    "        if isinstance(df, pd.Series):\n",
    "            return df\n",
    "        if len(df.columns) == 1:\n",
    "            return df[df.columns[0]].astype(str)\n",
    "\n",
    "        def serialize_row(row):\n",
    "            items = [(col, str(val) if not pd.isnull(val) else \"N/A\") for col, val in row.items()]\n",
    "            return \"\\n\\n\".join([f\"{col}: {val}\" for col, val in items])\n",
    "        return df.apply(serialize_row, axis=1)\n",
    "\n",
    "    def _generate_pairs(self, X, y):\n",
    "        classes = y.unique()\n",
    "        pairs = []\n",
    "        for i in range(len(classes)):\n",
    "            for j in range(i + 1, len(classes)):\n",
    "                class_a = classes[i]\n",
    "                class_b = classes[j]\n",
    "                ex_a = X[y == class_a]\n",
    "                ex_b = X[y == class_b]\n",
    "                for k in range(max(len(ex_a), len(ex_b))):\n",
    "                    pairs.append((\n",
    "                        ex_a.iloc[k % len(ex_a)] if k % 2 == 0 else ex_b.iloc[k % len(ex_b)],\n",
    "                        ex_b.iloc[k % len(ex_b)] if k % 2 == 0 else ex_a.iloc[k % len(ex_a)],\n",
    "                        class_a if k % 2 == 0 else class_b,\n",
    "                        class_b if k % 2 == 0 else class_a\n",
    "                    ))\n",
    "        return pairs\n",
    "\n",
    "    def _initialize_llm(self, llm_name, temperature):\n",
    "        return {\n",
    "            \"model\": llm_name,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "\n",
    "    def _generate_features_for_pair(self, llm, example_a, example_b, label_a, label_b):\n",
    "        \"\"\"\n",
    "        Метод генерации набора фич (features), с использованием Ollama\n",
    "        и структурированного вывода (JSON-схема).\n",
    "        \"\"\"\n",
    "        # Создаём пользовательский prompt\n",
    "        user_prompt = (\n",
    "            f\"У нас есть два примера из разных классов.\\n\"\n",
    "            f\"Класс A: {label_a}\\n\"\n",
    "            f\"Пример A:\\n{example_a}\\n\\n\"\n",
    "            f\"Класс B: {label_b}\\n\"\n",
    "            f\"Пример B:\\n{example_b}\\n\\n\"\n",
    "            \"Сгенерируй набор признаков (features), чтобы отличать класс A от класса B. \"\n",
    "            \"Верни результат строго в JSON по заданной схеме.\\n\"\n",
    "        )\n",
    "\n",
    "        # Выбираем схему (категориальные или числовые)\n",
    "        if self.discrete_features:\n",
    "            schema_str = CategoricalFeatureSet.schema_json()\n",
    "        else:\n",
    "            schema_str = NumericalFeatureSet.schema_json()\n",
    "\n",
    "        # --- Псевдокод вызова Ollama (зависит от вашей среды) ---\n",
    "        # Допустим, ollama.chat(...) вернёт объект с полем .content,\n",
    "        # содержащим JSON.\n",
    "        #\n",
    "        #  response = ollama.chat(\n",
    "        #      model=llm[\"model\"],\n",
    "        #      format=schema_str,\n",
    "        #      temperature=llm[\"temperature\"],\n",
    "        #      messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        #  )\n",
    "        #\n",
    "        # В демо-версии сделаем фейковый ответ:\n",
    "        fake_json = \"\"\"\n",
    "        {\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"name\": \"style\",\n",
    "                    \"possible_values\": [\"formal\", \"casual\", \"other\"],\n",
    "                    \"description\": \"описание стиля\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"sentiment\",\n",
    "                    \"possible_values\": [\"positive\", \"negative\", \"neutral\"],\n",
    "                    \"description\": \"общая тональность\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\" if self.discrete_features else \"\"\"\n",
    "        {\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"name\": \"length\",\n",
    "                    \"zero\": \"короткий текст\",\n",
    "                    \"ten\": \"очень длинный текст\",\n",
    "                    \"description\": \"примерная длина\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"sentiment_score\",\n",
    "                    \"zero\": \"полностью негативный\",\n",
    "                    \"ten\": \"полностью позитивный\",\n",
    "                    \"description\": \"оценка тональности\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # Предположим, в response.content лежит этот JSON\n",
    "        #  real_json_str = response.content\n",
    "        real_json_str = fake_json\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(real_json_str)\n",
    "            if self.discrete_features:\n",
    "                feature_set = CategoricalFeatureSet(**parsed)\n",
    "            else:\n",
    "                feature_set = NumericalFeatureSet(**parsed)\n",
    "        except Exception as e:\n",
    "            # Если вдруг невалидный JSON — вернём пустой набор\n",
    "            print(f\"Ошибка парсинга JSON: {e}\")\n",
    "            if self.discrete_features:\n",
    "                feature_set = CategoricalFeatureSet(features=[])\n",
    "            else:\n",
    "                feature_set = NumericalFeatureSet(features=[])\n",
    "\n",
    "        return feature_set, None\n",
    "\n",
    "    def _transform_example(self, llm, example, featureset):\n",
    "        \"\"\"\n",
    "        Метод, который \"скорит\" (аннотирует) пример по уже сгенерированным признакам.\n",
    "        Снова используем Ollama + JSON-схему (но теперь схему вида: {feature_name: value}).\n",
    "        \"\"\"\n",
    "        user_prompt = (\n",
    "            f\"Текст:\\n{example}\\n\\n\"\n",
    "            \"Задано несколько признаков (features). \"\n",
    "            \"Верни JSON, где ключи — имена фич, а значения — выбранная категория (для категориальных) \"\n",
    "            \"или целое число 0..10 (для числовых).\\n\\n\"\n",
    "            \"Список фич:\\n\"\n",
    "            f\"{featureset.json()}\\n\\n\"\n",
    "            \"Формат ответа: только JSON, без добавления объяснений.\"\n",
    "        )\n",
    "\n",
    "        # Псевдокод вызова Ollama:\n",
    "        #\n",
    "        #  response = ollama.chat(\n",
    "        #      model=llm[\"model\"],\n",
    "        #      messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        #      # Если Ollama поддерживает аналог формат=..., то задайте схему\n",
    "        #      # или верните просто JSON\n",
    "        #  )\n",
    "        #\n",
    "        # Для демонстрации — фейковый JSON:\n",
    "        fake_score_json = \"\"\"{\n",
    "            \"style\": \"other\",\n",
    "            \"sentiment\": \"positive\"\n",
    "        }\"\"\" if self.discrete_features else \"\"\"{\n",
    "            \"length\": 8,\n",
    "            \"sentiment_score\": 2\n",
    "        }\"\"\"\n",
    "\n",
    "        try:\n",
    "            scores = json.loads(fake_score_json)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка парсинга JSON при скоринге: {e}\")\n",
    "            scores = {}\n",
    "\n",
    "        # Минимальная валидация\n",
    "        if self.discrete_features:\n",
    "            for f in featureset.features:\n",
    "                if f.name not in scores:\n",
    "                    scores[f.name] = None\n",
    "                else:\n",
    "                    val = scores[f.name]\n",
    "                    # Если значение не в списке possible_values — сбрасываем\n",
    "                    if val not in f.possible_values:\n",
    "                        scores[f.name] = None\n",
    "        else:\n",
    "            for f in featureset.features:\n",
    "                if f.name not in scores:\n",
    "                    scores[f.name] = None\n",
    "                else:\n",
    "                    val = scores[f.name]\n",
    "                    if not isinstance(val, int):\n",
    "                        scores[f.name] = None\n",
    "                    elif val < 0 or val > 10:\n",
    "                        scores[f.name] = None\n",
    "\n",
    "        return scores, None\n",
    "\n",
    "    def _create_feature_embeddings(self, feature_set):\n",
    "        # Если бы мы использовали локальные эмбеддинги, здесь бы дергали нужный API.\n",
    "        # Для примера — возвращаем случайный вектор на 16 измерений\n",
    "        num_feats = len(feature_set.features)\n",
    "        if num_feats == 0:\n",
    "            return np.zeros((0, 16), dtype=float)\n",
    "        return np.random.rand(num_feats, 16)\n",
    "\n",
    "    def _cluster_features(self, feature_embeddings):\n",
    "        if len(feature_embeddings) == 0:\n",
    "            return np.array([], dtype=int)\n",
    "        self._hdbscan = HDBSCAN(\n",
    "            min_cluster_size=2,\n",
    "            allow_single_cluster=True,\n",
    "            cluster_selection_method=\"leaf\"\n",
    "        )\n",
    "        labels = self._hdbscan.fit_predict(feature_embeddings)\n",
    "        return labels\n",
    "\n",
    "    def _select_representative_features(self, feature_set, feature_embeddings, cluster_labels):\n",
    "        if len(feature_set.features) == 0:\n",
    "            return feature_set\n",
    "\n",
    "        unique_labels = np.unique(cluster_labels)\n",
    "        rep_features = []\n",
    "\n",
    "        for lbl in unique_labels:\n",
    "            idxs = np.where(cluster_labels == lbl)[0]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "            centroid = np.mean(feature_embeddings[idxs], axis=0)\n",
    "            # Находим ближайшую к центроиду\n",
    "            dists = np.sum((feature_embeddings[idxs] - centroid) ** 2, axis=1)\n",
    "            best_idx = idxs[np.argmin(dists)]\n",
    "            rep_features.append(feature_set.features[best_idx])\n",
    "\n",
    "        if self.discrete_features:\n",
    "            return CategoricalFeatureSet(features=rep_features)\n",
    "        else:\n",
    "            return NumericalFeatureSet(features=rep_features)\n",
    "\n",
    "    def _ensure_unique_feature_names(self, feature_set):\n",
    "        \"\"\"\n",
    "        При наличии дубликатов фичей (например, несколько 'sentiment') — добавляем суффикс _2, _3, ...\n",
    "        \"\"\"\n",
    "        name_counts = {}\n",
    "        for f in feature_set.features:\n",
    "            # Приведём к нижнему регистру, заменим пробелы на '_'\n",
    "            orig = f.name.lower().replace(\" \", \"_\")\n",
    "            if orig not in name_counts:\n",
    "                name_counts[orig] = 1\n",
    "                f.name = orig\n",
    "            else:\n",
    "                name_counts[orig] += 1\n",
    "                new_name = f\"{orig}_{name_counts[orig]}\"\n",
    "                f.name = new_name\n",
    "\n",
    "        self._global_duplicate_counter = sum(c - 1 for c in name_counts.values())\n",
    "        return feature_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nm7KMw8X_8w",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Zero-Shot GPT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9T51by2nXVWt"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from ollama import chat, ChatResponse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PromptingTQDM(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}{postfix}]\", *args, **kwargs)\n",
    "        self.reset_stats()\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.tokens_in = 0\n",
    "        self.tokens_out = 0\n",
    "        self.costs = 0\n",
    "        self.n_features = 0\n",
    "        self.n_iter = 0\n",
    "\n",
    "    def log_stats(self, tokens_in, tokens_out, cost, n_features):\n",
    "        self.tokens_in += tokens_in\n",
    "        self.tokens_out += tokens_out\n",
    "        self.costs += cost\n",
    "        self.n_features += n_features\n",
    "        self.n_iter += 1\n",
    "\n",
    "        avg_tokens_in = self.tokens_in / self.n_iter\n",
    "        avg_tokens_out = self.tokens_out / self.n_iter\n",
    "        avg_cost = self.costs / self.n_iter\n",
    "\n",
    "        self.set_postfix_str(f\"in {avg_tokens_in:.0f} tokens/{self.unit}, out {avg_tokens_out:.0f} tokens/{self.unit}, {avg_cost:.5f} USD/{self.unit}, {self.costs:.2f} USD total, {self.n_features:.0f} features total\", refresh=True)\n",
    "\n",
    "\n",
    "class ZeroShotOllamaCallback:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.consumption_log = []\n",
    "\n",
    "    def log_consumption(self, example: str, system_message: str, prompt_message: str, llm_output: str):\n",
    "        self.consumption_log.append({\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n",
    "            \"Example\": example,\n",
    "            \"System Message\": system_message,\n",
    "            \"Prompt Message\": prompt_message,\n",
    "            \"LLM Output\": llm_output,\n",
    "        })\n",
    "\n",
    "class ZeroShotGPTClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, context=None, llm_name=\"qwen2.5:14b\", verbose=False, callback=None):\n",
    "        \"\"\"\n",
    "        Initializes the classifier with the model and optional context for classification.\n",
    "        \"\"\"\n",
    "        self.context = context\n",
    "        self.model_name = llm_name\n",
    "        self.verbose = verbose\n",
    "        self.callback = callback\n",
    "\n",
    "        # Define the system prompt and classification instruction templates\n",
    "        self.system_prompt = (\n",
    "            \"You are a data classifier. Your task is to classify a given example into one of multiple classes.\"\n",
    "            f\"{' The concrete context is the following: ' + self.context if self.context else ''}\"\n",
    "        )\n",
    "        self.classification_prompt_template = \"\"\"\\\n",
    "##### Here is an example that you should classify #####\n",
    "\n",
    "{example}\n",
    "\n",
    "##### Instructions #####\n",
    "\n",
    "What is the most likely class of this example? Respond only with exactly one of the following class names and nothing else:\n",
    "{classes}.\n",
    "        \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the classifier by storing unique labels.\n",
    "        \"\"\"\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class for each example in X.\n",
    "        \"\"\"\n",
    "        # Ensure the classifier is fitted\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Initialize the prediction list\n",
    "        predictions = []\n",
    "\n",
    "        for example in X:\n",
    "            # Create the classification prompt\n",
    "            prompt = self.classification_prompt_template.format(\n",
    "                example=example,\n",
    "                classes=\", \".join([f\"'{cls}'\" for cls in self.classes_])\n",
    "            )\n",
    "\n",
    "            # Call the model using the ollama `chat` function\n",
    "            try:\n",
    "                response: ChatResponse = chat(\n",
    "                    model=self.model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Extract and clean the response content\n",
    "                prediction = response.message.content.strip()\n",
    "                if prediction in self.classes_:\n",
    "                    predictions.append(prediction)\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid prediction '{prediction}'.\")\n",
    "                    predictions.append(None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction: {e}\")\n",
    "                predictions.append(None)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30UBAYwNAcE1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PcItsccUAf7E"
   },
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# from transformers import Trainer, TrainingArguments, set_seed\n",
    "# import torch\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Class which iteratively returns a model input string as a tensor\n",
    "# class TorchDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "#         item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "# class FineTunedClassificationLLM(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "#     def __init__(self, model_name=\"FacebookAI/roberta-base\", max_tokens=512, epochs=1, model_path=\"./models/\", output_path=\"./results\", logging_path=\"./logs\", seed=42):\n",
    "#         self.model_name = model_name\n",
    "#         self.max_tokens = max_tokens\n",
    "#         self.epochs = epochs\n",
    "#         self.model_path = model_path\n",
    "#         self.output_path = output_path\n",
    "#         self.logging_path = logging_path\n",
    "#         self.seed = seed\n",
    "\n",
    "#         self.training_args = TrainingArguments(\n",
    "#             seed = self.seed,                   # Random seed for initialization\n",
    "#             output_dir=self.output_path,        # Directory for storing model predictions and checkpoints\n",
    "#             logging_dir=self.logging_path,      # Directory for storing Tensorboard logs\n",
    "#             num_train_epochs=self.epochs,       # Number of training epochs to perform\n",
    "#             per_device_train_batch_size=16,     # Batch size per GPU/TPU core/CPU for training\n",
    "#             learning_rate = 5e-5,               # Initial learning rate for Adam\n",
    "#             fp16 = True,                        # Use 16-bit (mixed) precision training (through NVIDIA apex)\n",
    "#         )\n",
    "\n",
    "#         if not torch.cuda.is_available():\n",
    "#             print(\"Warning: Cuda is not available on this hardware. Training an LLM may run into problems.\")\n",
    "\n",
    "#         # Set the seed for model initialization (for reproducible initial weights of the fully-connected classification layer)\n",
    "#         set_seed(self.seed)\n",
    "\n",
    "#         # Load model and tokenizer\n",
    "#         self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=2).to(\"cuda\")\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         # Remember the unique class labels\n",
    "#         self.classes_ = list(unique_labels(y))\n",
    "#         if len(self.classes_) != 2:\n",
    "#             raise ValueError(f\"FineTunedClassificationLLM only supports binary classification but found {len(self.classes_)} unique classes.\")\n",
    "\n",
    "#         # Map class labels to 0 and 1\n",
    "#         y = [self.classes_.index(label) for label in y]\n",
    "\n",
    "#         # Convert input data to list, if necessary (e.g., when data is provided as a Pandas Series)\n",
    "#         if not isinstance(X, list):\n",
    "#             X = list(X)\n",
    "\n",
    "#         # Tokenize the data\n",
    "#         train_encodings = self.tokenizer(X, padding=True, truncation=True, max_length=self.max_tokens)\n",
    "\n",
    "#         # Convert tokenized data into a torch Dataset\n",
    "#         train_dataset = TorchDataset(train_encodings, y)\n",
    "\n",
    "#         # Train the model\n",
    "#         trainer = Trainer(\n",
    "#             model=self.model,                    # the instantiated Transformers model to be trained\n",
    "#             args=self.training_args,             # training arguments which are defined above\n",
    "#             train_dataset=train_dataset,         # training dataset\n",
    "#         )\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save the model\n",
    "#         self.model.save_pretrained(self.model_path)\n",
    "#         self.tokenizer.save_pretrained(self.model_path)\n",
    "\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         preds = []\n",
    "#         for x in X:\n",
    "#             # Tokenize the example\n",
    "#             inputs = self.tokenizer(x, padding=True, truncation=True, max_length=self.max_tokens, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#             # Perform inference with the model\n",
    "#             outputs = self.model(**inputs)\n",
    "\n",
    "#             # Convert into the predicted class label\n",
    "#             pred = self.classes_[outputs[0].softmax(1).argmax()]\n",
    "#             preds.append(pred)\n",
    "\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRfT8MeGncGM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KYQ3E2dtqPCm"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import KNNImputer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Если класс FELIX лежит в другом модуле, импортируйте отсюда:\n",
    "# from felix_ollama import FELIX\n",
    "\n",
    "\n",
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    Класс для различных преобразований данных:\n",
    "    1. Сырой текст (get_raw)\n",
    "    2. TF-IDF векторизация (get_tfidf)\n",
    "    3. Генерация эмбеддингов через Transformers (get_embeddings)\n",
    "    4. Генерация фич с помощью FELIX (get_felix)\n",
    "    5. One-hot кодирование (one_hot_encode)\n",
    "    6. Импьютация пропусков (impute_missing_values)\n",
    "    7. Возврат последней \"модели\" (get_model_instance)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, language=\"english\"):\n",
    "        \"\"\"\n",
    "        dataset: объект с атрибутами X_train, X_test, context_train, context_test и т. д.\n",
    "        language: язык текстов (по умолчанию 'english'). Можно указать 'russian' и т.д.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.last_instance = None  # Reference to the last trained model instance\n",
    "        self.language = language.lower().strip()  # например, 'english' или 'russian'\n",
    "\n",
    "    # =========================================================================\n",
    "    # 1) Сырой текст\n",
    "    # =========================================================================\n",
    "    def get_raw(self):\n",
    "        \"\"\"\n",
    "        Возвращает тексты в виде Series (одна колонка \"Raw Text\"),\n",
    "        объединив несколько колонок датасета (если их несколько) в одну строку.\n",
    "        \"\"\"\n",
    "        X_train_raw = self._serialize_dataframe(self.dataset.X_train)\n",
    "        X_test_raw = self._serialize_dataframe(self.dataset.X_test)\n",
    "\n",
    "        X_train_raw.name = \"Raw Text\"\n",
    "        X_test_raw.name = \"Raw Text\"\n",
    "\n",
    "        self.last_instance = None\n",
    "        return X_train_raw, X_test_raw\n",
    "\n",
    "    # =========================================================================\n",
    "    # 2) TF-IDF векторизация\n",
    "    # =========================================================================\n",
    "    def get_tfidf(self):\n",
    "        \"\"\"\n",
    "        Преобразует тексты в TF-IDF вектор, учитывая стоп-слова и стемминг.\n",
    "        По умолчанию язык — self.language (english или russian), но можно расширить.\n",
    "        \"\"\"\n",
    "        # Сырой текст (Series)\n",
    "        X_train_raw, X_test_raw = self.get_raw()\n",
    "\n",
    "        # Проверяем нужные ресурсы\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "        # Инициализируем список стоп-слов в зависимости от языка\n",
    "        try:\n",
    "            stop_words = set(stopwords.words(self.language))\n",
    "        except OSError:\n",
    "            # Если NLTK не содержит стоп-слов для данного языка, используем пустой набор\n",
    "            print(f\"[Warning] No stopwords found for language='{self.language}'. Using empty list.\")\n",
    "            stop_words = set()\n",
    "\n",
    "        # Инициализируем стеммер\n",
    "        #  - Для английского: PorterStemmer()\n",
    "        #  - Для русского: SnowballStemmer(\"russian\")\n",
    "        #  - Если язык не поддерживается, используем \"universal\" логику (без стемминга)\n",
    "        stemmer = None\n",
    "        if self.language == \"english\":\n",
    "            stemmer = PorterStemmer()\n",
    "        elif self.language == \"russian\":\n",
    "            stemmer = SnowballStemmer(\"russian\")\n",
    "        else:\n",
    "            print(f\"[Warning] Stemming for language='{self.language}' is not implemented. No stemming applied.\")\n",
    "\n",
    "        def tokenize_and_stem(text):\n",
    "            # Токенизация\n",
    "            tokens = [word.lower() for word in word_tokenize(text) if word.isalpha()]\n",
    "            # Удаляем стоп-слова\n",
    "            filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "            # Стемминг (если мы знаем стеммер)\n",
    "            if stemmer:\n",
    "                return [stemmer.stem(t) for t in filtered_tokens]\n",
    "            else:\n",
    "                return filtered_tokens\n",
    "\n",
    "        # Создаём TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem)\n",
    "\n",
    "        # Обучаем на train и трансформируем train/test\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "        X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "\n",
    "        self.last_instance = vectorizer  # сохраним для доступа\n",
    "\n",
    "        # Превращаем разреженные матрицы в DataFrame\n",
    "        X_train_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "            X_train_tfidf, columns=vectorizer.get_feature_names_out()\n",
    "        )\n",
    "        X_test_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "            X_test_tfidf, columns=vectorizer.get_feature_names_out()\n",
    "        )\n",
    "        return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "    # =========================================================================\n",
    "    # 3) Генерация эмбеддингов через Transformers\n",
    "    # =========================================================================\n",
    "    def get_embeddings(self, model_name='intfloat/multilingual-e5-large'):\n",
    "        \"\"\"\n",
    "        Генерирует эмбеддинги для текстов (Train/Test), используя HuggingFace Transformers.\n",
    "        По умолчанию модель 'intfloat/multilingual-e5-large' (поддерживает много языков).\n",
    "        \"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # from_pretrained(...) не всегда принимает параметр device=device напрямую,\n",
    "        # поэтому уточним логику:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Локальная функция для генерации эмбеддингов\n",
    "        def compute_embeddings(texts):\n",
    "            embeddings = []\n",
    "            for text in tqdm(texts, desc=f\"Generating embeddings ({model_name})\"):\n",
    "                inputs = tokenizer(\n",
    "                    text, return_tensors='pt', truncation=True, padding=True\n",
    "                ).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    # Mean pooling over the token embeddings\n",
    "                    emb = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "                    embeddings.append(emb.cpu().numpy())\n",
    "            return embeddings\n",
    "\n",
    "        # Берём сырой текст\n",
    "        X_train_raw, X_test_raw = self.get_raw()\n",
    "\n",
    "        # Генерируем эмбеддинги\n",
    "        X_train_embeddings = compute_embeddings(X_train_raw)\n",
    "        X_test_embeddings = compute_embeddings(X_test_raw)\n",
    "\n",
    "        # Проверяем, что не пусто\n",
    "        if len(X_train_embeddings) == 0 or len(X_test_embeddings) == 0:\n",
    "            raise ValueError(\"Embeddings are empty. Check your data or the model configuration.\")\n",
    "\n",
    "        # Преобразуем в DataFrame\n",
    "        dim_size = len(X_train_embeddings[0])\n",
    "        X_train_embeddings = pd.DataFrame(X_train_embeddings, columns=[f\"Dim_{i}\" for i in range(dim_size)])\n",
    "        X_test_embeddings = pd.DataFrame(X_test_embeddings, columns=[f\"Dim_{i}\" for i in range(dim_size)])\n",
    "\n",
    "        self.last_instance = model\n",
    "        return X_train_embeddings, X_test_embeddings\n",
    "\n",
    "    # =========================================================================\n",
    "    # 4) Генерация фич с помощью FELIX\n",
    "    # =========================================================================\n",
    "    def get_felix(self, discrete_features=False, gpt4=False, callback=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Вызывает FELIX для генерации фич, используя локальную Qwen-модель (через Ollama).\n",
    "        Параметр gpt4 — формальный (если решите снова переключаться).\n",
    "        \"\"\"\n",
    "        # Пока жёстко зашиваем qwen2.5:14b\n",
    "        llm_name = \"qwen2.5:14b\"\n",
    "\n",
    "        self.last_instance = FELIX(\n",
    "            context=self.dataset.context_train,\n",
    "            llm_name=llm_name,\n",
    "            discrete_features=discrete_features,\n",
    "            zero_shot=self.dataset.train_zero_shot,\n",
    "            callback=callback,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        # Обучаем FELIX (fit) и сразу трансформируем (train)\n",
    "        X_train_felix = self.last_instance.fit_transform(self.dataset.X_train, self.dataset.y_train)\n",
    "\n",
    "        # Переключаем контекст на тестовый\n",
    "        self.last_instance.context = self.dataset.context_test\n",
    "        X_test_felix = self.last_instance.transform(self.dataset.X_test)\n",
    "\n",
    "        return X_train_felix, X_test_felix\n",
    "\n",
    "    # =========================================================================\n",
    "    # 5) One-hot кодирование\n",
    "    # =========================================================================\n",
    "    def one_hot_encode(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Выполняет one-hot encoding, конкатенируя train и test (чтобы не было\n",
    "        несогласованных колонок).\n",
    "        \"\"\"\n",
    "        X_concat = pd.concat([X_train, X_test], axis=0)\n",
    "        X_concat_ohe = pd.get_dummies(X_concat, drop_first=False)\n",
    "        X_train_ohe = X_concat_ohe.iloc[: len(X_train), :]\n",
    "        X_test_ohe = X_concat_ohe.iloc[len(X_train):, :]\n",
    "        return X_train_ohe, X_test_ohe\n",
    "\n",
    "    # =========================================================================\n",
    "    # 6) Импьютация пропусков (KNN)\n",
    "    # =========================================================================\n",
    "    def impute_missing_values(self, X_train, X_test, col_drop_thres=0.1, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Удаляем колонки, где > col_drop_thres пропусков, затем KNN-Imputer.\n",
    "        По итогам значения округляются до int.\n",
    "        \"\"\"\n",
    "        def _impute(df):\n",
    "            # Удаляем колонки с высоким процентом пропусков\n",
    "            missing_perc = df.isnull().mean()\n",
    "            cols_to_drop = missing_perc[missing_perc > col_drop_thres].index\n",
    "            df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n",
    "            # c pandas output=True\n",
    "            imputer.set_output(transform=\"pandas\")\n",
    "            df_out = imputer.fit_transform(df)\n",
    "            # Округлим и приведём к int\n",
    "            df_out = df_out.round().astype(int)\n",
    "            return df_out\n",
    "\n",
    "        return _impute(X_train), _impute(X_test)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 7) Возврат последнего \"моделя\"\n",
    "    # =========================================================================\n",
    "    def get_model_instance(self):\n",
    "        \"\"\"\n",
    "        Возвращает последний объект, с которым работал DataTransformer:\n",
    "        - TF-IDF vectorizer\n",
    "        - Модель от HuggingFace\n",
    "        - Объект FELIX\n",
    "        - или None (если ничего не было)\n",
    "        \"\"\"\n",
    "        return self.last_instance\n",
    "\n",
    "    # =========================================================================\n",
    "    # Вспомогательный метод сериализации DataFrame -> Series\n",
    "    # =========================================================================\n",
    "    def _serialize_dataframe(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Превращает DataFrame (возможно с несколькими столбцами) в Series строк,\n",
    "        склеивая их для каждой строки (с подстановкой 'N/A' на пропуски).\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.Series):\n",
    "            return df\n",
    "\n",
    "        # Если всего один столбец, просто приведение к str\n",
    "        if df.shape[1] == 1:\n",
    "            return df[df.columns[0]].astype(str)\n",
    "\n",
    "        # Если несколько столбцов - склеиваем\n",
    "        def row_to_text(row):\n",
    "            items = []\n",
    "            for col, val in row.items():\n",
    "                if pd.isnull(val):\n",
    "                    val_str = \"N/A\"\n",
    "                else:\n",
    "                    val_str = str(val)\n",
    "                # Можно, например, вставлять \"col: val\"\n",
    "                items.append(f\"{col}: {val_str}\")\n",
    "            return \"\\n\\n\".join(items)\n",
    "\n",
    "        return df.apply(row_to_text, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xxjjKOXG9ss",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run Tracking Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_5g7rgIuIOc"
   },
   "source": [
    "Set your Weights & Biases project in lines 21 and 22 of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cFkm0OU5HB2W"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb  # Предполагаю, что вы используете Weights & Biases\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Если у вас где-то определены:\n",
    "# from zero_shot_gpt_classifier import ZeroShotGPTClassifier, ZeroShotOllamaCallback\n",
    "# from felix_ollama import FELIX, FELIXCallback, NumericalFeatureSet, CategoricalFeatureSet\n",
    "\n",
    "# Важно: импортируем ваш DataTransformer (новый) из соответствующего модуля\n",
    "# from data_transformer import DataTransformer\n",
    "\n",
    "\n",
    "class Run:\n",
    "    def __init__(self, data_representation, classifier, dataset, seed=0, language=\"english\"):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "        1) data_representation: строка или список строк, описывающих тип преобразования данных\n",
    "           (например, \"TF-IDF\", \"Embeddings\", \"Raw Text\", \"FELIX qwen (Numerical)\", ...)\n",
    "        2) classifier: строка или список строк, описывающих классификатор (\"RandomForest\", \"LogisticRegression\", ...)\n",
    "        3) dataset: объект датасета, содержащий X_train, y_train, X_test, y_test, context_train, context_test и т. д.\n",
    "        4) seed: random_state\n",
    "        5) language: язык данных для DataTransformer (например, \"english\" или \"russian\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Приводим к списку, если пришла одна строка\n",
    "        self.data_representation = (\n",
    "            [data_representation] if isinstance(data_representation, str) else data_representation\n",
    "        )\n",
    "        self.classifier = [classifier] if isinstance(classifier, str) else classifier\n",
    "        self.dataset = dataset\n",
    "        self.seed = seed\n",
    "\n",
    "        # Создаём новый DataTransformer\n",
    "        self.data_transformer = DataTransformer(dataset=dataset, language=language)\n",
    "\n",
    "        # Кешируем результаты трансформации, чтобы при повторных вызовах того же data_representation\n",
    "        # не пересчитывать заново\n",
    "        self.cache = {}\n",
    "\n",
    "        # Параметры для Weights & Biases (W&B)\n",
    "        self.wandb_entity = \"mkgs210-itmo-university\"\n",
    "        self.wandb_project = \"felix\"\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Запускает single_run() для каждой комбинации (data_representation, classifier).\"\"\"\n",
    "        for dr in self.data_representation:\n",
    "            for cls in self.classifier:\n",
    "                self.single_run(dr, cls, self.dataset)\n",
    "\n",
    "    def single_run(self, data_representation, classifier, dataset):\n",
    "        print(f\"Starting new run '{data_representation} {classifier}' on '{dataset.id}' ...\")\n",
    "\n",
    "        # Инициализируем W&B для логгирования\n",
    "        self.init_logging(data_representation, classifier, dataset)\n",
    "\n",
    "        # Проверяем, нет ли уже готовых результатов для этой data_representation\n",
    "        if data_representation in self.cache:\n",
    "            print(f\"Loading cached data transformation '{data_representation}' ...\")\n",
    "            (\n",
    "                X_train_transformed,\n",
    "                X_test_transformed,\n",
    "                transformer_instance,\n",
    "                callback,\n",
    "                time_transformation\n",
    "            ) = self.cache[data_representation]\n",
    "        else:\n",
    "            # Иначе выполняем transform_data\n",
    "            print(f\"Transforming data using '{data_representation}' ...\")\n",
    "            start_time = time.time()\n",
    "            (\n",
    "                X_train_transformed,\n",
    "                X_test_transformed,\n",
    "                transformer_instance,\n",
    "                callback\n",
    "            ) = self.transform_data(data_representation)\n",
    "            time_transformation = time.time() - start_time\n",
    "\n",
    "            # Кладём всё в кэш\n",
    "            self.cache[data_representation] = (\n",
    "                X_train_transformed,\n",
    "                X_test_transformed,\n",
    "                transformer_instance,\n",
    "                callback,\n",
    "                time_transformation\n",
    "            )\n",
    "\n",
    "        print(\"X_train.shape =\", X_train_transformed.shape)\n",
    "        print(\"X_test.shape =\", X_test_transformed.shape)\n",
    "\n",
    "        # Сохраняем копии данных до one-hot/imputation (для логирования)\n",
    "        X_train_transformed_original = X_train_transformed\n",
    "        X_test_transformed_original = X_test_transformed\n",
    "\n",
    "        # Если надо, делаем one-hot (категориальные фичи FELIX)\n",
    "        if data_representation in [\"FELIX qwen (Categorical)\"]:\n",
    "            print(\"Creating one-hot encoding of categorical data ...\")\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.one_hot_encode(\n",
    "                X_train_transformed, X_test_transformed\n",
    "            )\n",
    "\n",
    "        # Или KNN imputation (числовые фичи FELIX)\n",
    "        elif data_representation in [\"FELIX qwen (Numerical)\"]:\n",
    "            print(\"Imputing missing values in numerical data ...\")\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.impute_missing_values(\n",
    "                X_train_transformed, X_test_transformed\n",
    "            )\n",
    "\n",
    "        # Обучаем классификатор, делаем предсказания\n",
    "        start_time = time.time()\n",
    "        print(f\"Training '{classifier}' on '{dataset.id}' training set ...\")\n",
    "        self.classifier_instance = self.get_classifier(classifier)\n",
    "\n",
    "        self.classifier_instance.fit(X_train_transformed, dataset.y_train)\n",
    "\n",
    "        print(f\"Predicting class labels of '{dataset.id}' test set with trained '{classifier}' ...\")\n",
    "        self.y_pred = self.classifier_instance.predict(X_test_transformed)\n",
    "        time_classification = time.time() - start_time\n",
    "\n",
    "        # Если бы у вас был ZeroShotGPTClassifier, то тут можно было бы\n",
    "        # обработать случаи, когда предсказанный класс не входит в набор y_test\n",
    "\n",
    "        # Логгируем результаты\n",
    "        print(\"Finishing run and logging the results ...\")\n",
    "        self.log(\n",
    "            y_pred=self.y_pred,\n",
    "            X_train_transformed=X_train_transformed_original,\n",
    "            X_test_transformed=X_test_transformed_original,\n",
    "            classifier_instance=self.classifier_instance,\n",
    "            transformer_instance=transformer_instance,\n",
    "            callback=callback,\n",
    "            time_transformation=time_transformation,\n",
    "            time_classification=time_classification\n",
    "        )\n",
    "        self.finish()\n",
    "\n",
    "    def transform_data(self, data_representation):\n",
    "        \"\"\"\n",
    "        Вспомогательный метод, который выбирает нужную \"ветку\" преобразования\n",
    "        (TF-IDF, Embeddings, FELIX...) и возвращает X_train_transformed, X_test_transformed,\n",
    "        transformer_instance, callback.\n",
    "        \"\"\"\n",
    "        if data_representation == \"TF-IDF\":\n",
    "            callback = None\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.get_tfidf()\n",
    "\n",
    "        elif data_representation == \"Embeddings\":\n",
    "            callback = None\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.get_embeddings()\n",
    "\n",
    "        elif data_representation == \"Raw Text\":\n",
    "            callback = None\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.get_raw()\n",
    "\n",
    "        elif data_representation == \"FELIX qwen (Numerical)\":\n",
    "            # Предположим, вы хотите отлавливать логи FELIX (LLM cost)\n",
    "            callback = CustomFELIXCallback()\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.get_felix(\n",
    "                discrete_features=False,\n",
    "                gpt4=False,\n",
    "                callback=callback,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "        elif data_representation == \"FELIX qwen (Categorical)\":\n",
    "            # Аналогично\n",
    "            callback = CustomFELIXCallback()\n",
    "            X_train_transformed, X_test_transformed = self.data_transformer.get_felix(\n",
    "                discrete_features=True,\n",
    "                gpt4=False,\n",
    "                callback=callback,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid data representation: {data_representation}\")\n",
    "\n",
    "        # Получаем \"трансформер\" из DataTransformer — это может быть TF-IDF vectorizer или FELIX, etc.\n",
    "        transformer_instance = self.data_transformer.get_model_instance()\n",
    "        return X_train_transformed, X_test_transformed, transformer_instance, callback\n",
    "\n",
    "    def get_classifier(self, classifier):\n",
    "        \"\"\"\n",
    "        Создаёт экземпляр нужного классификатора.\n",
    "        \"\"\"\n",
    "        if classifier == \"RandomForest\":\n",
    "            return RandomForestClassifier(random_state=self.seed)\n",
    "        elif classifier == \"LogisticRegression\":\n",
    "            return LogisticRegression(max_iter=10000, random_state=self.seed)\n",
    "        elif classifier == \"qwen\":\n",
    "            # Если у вас действительно есть ZeroShotGPTClassifier ...\n",
    "            return ZeroShotGPTClassifier(llm_name=\"qwen2.5:14b\", callback=ZeroShotOllamaCallback(), verbose=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value '{classifier}' for classifier.\")\n",
    "\n",
    "    def init_logging(self, data_representation, classifier, dataset):\n",
    "        \"\"\"\n",
    "        Инициализируем WandB. Можете убрать, если не используете.\n",
    "        \"\"\"\n",
    "\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            entity=self.wandb_entity,\n",
    "            project=self.wandb_project,\n",
    "            name=f\"{data_representation} {classifier}\",\n",
    "            config={\n",
    "                \"dataset\": dataset.id,\n",
    "                \"dataset short name\": dataset.short_name,\n",
    "                \"data_representation\": data_representation,\n",
    "                \"model\": classifier,\n",
    "                \"context_description\": dataset.context_train,\n",
    "                \"context_description_test\": dataset.context_test,\n",
    "                \"n_train\": dataset.X_train.shape[0],\n",
    "                \"n_test\": dataset.X_test.shape[0],\n",
    "                \"seed\": self.seed,\n",
    "                \"pos_class\": dataset.pos_class,\n",
    "                \"train_zero_shot\": dataset.train_zero_shot\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def log(\n",
    "        self,\n",
    "        y_pred,\n",
    "        X_train_transformed,\n",
    "        X_test_transformed,\n",
    "        classifier_instance=None,\n",
    "        transformer_instance=None,\n",
    "        callback=None,\n",
    "        time_transformation=-1.0,\n",
    "        time_classification=-1.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Логгируем метрики, LLM cost (если есть), важность фич и т.д.\n",
    "        \"\"\"\n",
    "        # Считаем основные метрики\n",
    "        results = self.calculate_scores(self.dataset.y_test, y_pred, pos_class=self.dataset.pos_class)\n",
    "        results[\"Time Transformation\"] = time_transformation\n",
    "        results[\"Time Classification\"] = time_classification\n",
    "        results[\"Time Total\"] = time_transformation + time_classification\n",
    "\n",
    "        # Если есть callback от FELIX, логгируем cost\n",
    "        if callback and isinstance(callback, FELIXCallback):\n",
    "            generation_cost = sum([x[\"Total Cost\"] for x in callback.generation_log])\n",
    "            scoring_cost = sum([x[\"Total Cost\"] for x in callback.scoring_log])\n",
    "            total_cost = generation_cost + scoring_cost\n",
    "            results[\"Generation Cost\"] = generation_cost\n",
    "            results[\"Scoring Cost\"] = scoring_cost\n",
    "            results[\"Total Cost\"] = total_cost\n",
    "        elif callback and isinstance(callback, ZeroShotOllamaCallback):\n",
    "            # Если у вас ZeroShotOllamaCallback\n",
    "            results[\"Total Cost\"] = sum([x[\"Total Cost\"] for x in callback.consumption_log])\n",
    "\n",
    "        # Логгируем логи запросов FELIX\n",
    "        if callback and isinstance(callback, FELIXCallback):\n",
    "            results[\"Generation Log\"] = wandb.Table(dataframe=pd.DataFrame(callback.generation_log))\n",
    "            results[\"Scoring Log\"] = wandb.Table(dataframe=pd.DataFrame(callback.scoring_log))\n",
    "        elif callback and isinstance(callback, ZeroShotOllamaCallback):\n",
    "            # Для ZeroShot\n",
    "            results[\"Prompt Log\"] = wandb.Table(dataframe=pd.DataFrame(callback.consumption_log))\n",
    "\n",
    "        # Если transformer_instance — это FELIX, можем вывести описание фич\n",
    "        if transformer_instance and isinstance(transformer_instance, FELIX):\n",
    "            try:\n",
    "                df_features = transformer_instance.get_features_as_dataframe().rename(columns={\"Name\": \"Feature Name\"})\n",
    "            except ValueError:\n",
    "                df_features = None\n",
    "        else:\n",
    "            df_features = None\n",
    "\n",
    "        # Если используем классические модели (RF или LR), логгируем их важность/коэффициенты\n",
    "        if isinstance(classifier_instance, RandomForestClassifier):\n",
    "            # feature_names_in_ появилось в sklearn 1.0; если у вас старая версия, может не быть\n",
    "            df_importances = pd.DataFrame({\n",
    "                \"Feature Name\": classifier_instance.feature_names_in_,\n",
    "                \"Importance\": classifier_instance.feature_importances_\n",
    "            }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "            # Если FELIX дискретный + one-hot, может быть полезно разбить feature_name\n",
    "            if transformer_instance and isinstance(transformer_instance, FELIX) and transformer_instance.discrete_features:\n",
    "                df_importances[[\"Feature Name\", \"Value\"]] = df_importances[\"Feature Name\"].str.rsplit(\"_\", n=1, expand=True)\n",
    "\n",
    "            if isinstance(df_features, pd.DataFrame):\n",
    "                df_importances = df_importances.set_index(\"Feature Name\").join(\n",
    "                    df_features.set_index(\"Feature Name\"), how=\"left\"\n",
    "                ).reset_index()\n",
    "            results[\"Feature Importance\"] = df_importances\n",
    "\n",
    "        elif isinstance(classifier_instance, LogisticRegression):\n",
    "            df_coefficients = pd.DataFrame({\n",
    "                \"Feature Name\": classifier_instance.feature_names_in_,\n",
    "                \"Coefficient\": classifier_instance.coef_[0]\n",
    "            }).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "            if (\n",
    "                transformer_instance\n",
    "                and isinstance(transformer_instance, FELIX)\n",
    "                and transformer_instance.discrete_features\n",
    "            ):\n",
    "                df_coefficients[[\"Feature Name\", \"Value\"]] = df_coefficients[\"Feature Name\"].str.rsplit(\"_\", n=1, expand=True)\n",
    "\n",
    "            if isinstance(df_features, pd.DataFrame):\n",
    "                df_coefficients = df_coefficients.set_index(\"Feature Name\").join(\n",
    "                    df_features.set_index(\"Feature Name\"), how=\"left\"\n",
    "                ).reset_index()\n",
    "            results[\"Coefficients\"] = df_coefficients\n",
    "\n",
    "        # Сохраняем FELIX-инстанс (сериализуем) для отладки\n",
    "        if transformer_instance and isinstance(transformer_instance, FELIX):\n",
    "            filename = f\"FELIX-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
    "            transformer_instance.save_instance(filename)\n",
    "            #wandb.save(filename, policy=\"now\")\n",
    "\n",
    "        # Логгируем метрики\n",
    "        wandb.log(results)\n",
    "\n",
    "        # Логируем DataFrame с train/test (если хотим видеть как они выглядят)\n",
    "        try:\n",
    "            wandb.log({\n",
    "                \"Training Data\": wandb.Table(\n",
    "                    dataframe=pd.concat([\n",
    "                        pd.Series(self.dataset.y_train, name=\"Ground Truth\").reset_index(drop=True),\n",
    "                        self.dataset.X_train.reset_index(drop=True),\n",
    "                        X_train_transformed.reset_index(drop=True)\n",
    "                    ], axis=1)\n",
    "                ),\n",
    "                \"Test Data\": wandb.Table(\n",
    "                    dataframe=pd.concat([\n",
    "                        pd.Series(self.dataset.y_test, name=\"Ground Truth\").reset_index(drop=True),\n",
    "                        pd.Series(y_pred, name=\"Prediction\").reset_index(drop=True),\n",
    "                        self.dataset.X_test.reset_index(drop=True),\n",
    "                        X_test_transformed.reset_index(drop=True)\n",
    "                    ], axis=1)\n",
    "                )\n",
    "            })\n",
    "        except TypeError as e:\n",
    "            print(e)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def calculate_scores(self, y_true, y_pred, pos_class):\n",
    "        \"\"\"\n",
    "        Рассчитывает метрики (Accuracy, Balanced Accuracy, F1 и т.д.).\n",
    "        pos_class — метка «положительного» класса, если нужно.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "            \"F1 Score (Macro)\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"F1 Score (Positive Class)\": f1_score(y_true, y_pred, pos_label=pos_class),\n",
    "            \"Precision (Macro)\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"Precision (Positive Class)\": precision_score(y_true, y_pred, pos_label=pos_class),\n",
    "            \"Recall (Macro)\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"Recall (Positive Class)\": recall_score(y_true, y_pred, pos_label=pos_class)\n",
    "        }\n",
    "\n",
    "    def finish(self):\n",
    "        \"\"\"Завершаем WandB-сессию.\"\"\"\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Callback для FELIX, если нужно логгировать фичи/стоимость\n",
    "# =============================================================================\n",
    "class CustomFELIXCallback(FELIXCallback):\n",
    "    def __init__(self):\n",
    "        self.generation_log = []\n",
    "        self.scoring_log = []\n",
    "        self.error_log = []\n",
    "\n",
    "    def features_generated_for_pair(\n",
    "        self,\n",
    "        llm,\n",
    "        example_a: str,\n",
    "        example_b: str,\n",
    "        label_a: str,\n",
    "        label_b: str,\n",
    "        system_message: str,\n",
    "        prompt_message: str,\n",
    "        llm_output: str,\n",
    "        features: NumericalFeatureSet | CategoricalFeatureSet,\n",
    "        total_cost,\n",
    "        total_tokens,\n",
    "        prompt_tokens,\n",
    "        completion_tokens\n",
    "    ):\n",
    "        self.generation_log.append({\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n",
    "            \"LLM\": llm.model_name if hasattr(llm, \"model_name\") else str(llm),\n",
    "            \"Temperature\": llm.temperature if hasattr(llm, \"temperature\") else None,\n",
    "            \"Example A\": example_a,\n",
    "            \"Label A\": label_a,\n",
    "            \"Example B\": example_b,\n",
    "            \"Label B\": label_b,\n",
    "            \"System Message\": system_message,\n",
    "            \"Prompt Message\": prompt_message,\n",
    "            \"LLM Output\": llm_output,\n",
    "            \"Features\": str(features.features),\n",
    "            \"Total Cost\": total_cost,\n",
    "            \"Total Tokens\": total_tokens,\n",
    "            \"Prompt Tokens\": prompt_tokens,\n",
    "            \"Completion Tokens\": completion_tokens\n",
    "        })\n",
    "\n",
    "    def example_transformed(\n",
    "        self,\n",
    "        llm,\n",
    "        example: str,\n",
    "        feature_set: NumericalFeatureSet | CategoricalFeatureSet,\n",
    "        system_message: str,\n",
    "        prompt_message: str,\n",
    "        llm_output: str,\n",
    "        scores,\n",
    "        total_cost: float,\n",
    "        total_tokens: int,\n",
    "        prompt_tokens: int,\n",
    "        completion_tokens: int\n",
    "    ):\n",
    "        self.scoring_log.append({\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n",
    "            \"LLM\": llm.model_name if hasattr(llm, \"model_name\") else str(llm),\n",
    "            \"Temperature\": llm.temperature if hasattr(llm, \"temperature\") else None,\n",
    "            \"Example\": example,\n",
    "            \"Features\": str(feature_set.features),\n",
    "            \"System Message\": system_message,\n",
    "            \"Prompt Message\": prompt_message,\n",
    "            \"LLM Output\": llm_output,\n",
    "            \"Scores\": str(scores),\n",
    "            \"Total Cost\": total_cost,\n",
    "            \"Total Tokens\": total_tokens,\n",
    "            \"Prompt Tokens\": prompt_tokens,\n",
    "            \"Completion Tokens\": completion_tokens\n",
    "        })\n",
    "\n",
    "    def error_encountered(self, llm, system_message, prompt_message, llm_output, error):\n",
    "        self.error_log.append({\n",
    "            \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d, %H:%M:%S\"),\n",
    "            \"LLM\": str(llm),\n",
    "            \"System Message\": system_message,\n",
    "            \"Prompt Message\": prompt_message,\n",
    "            \"LLM Output\": llm_output,\n",
    "            \"Error\": error\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FqArEL7qW7zV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Experiment 1: Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uqJZcVqnTOYl",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='amazon_polarity'\n",
       "\tshort_name='reviews-amazon'\n",
       "\tcontext_train='Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tcontext_test='Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tclasses=['POSITIVE', 'NEGATIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100, 2)\n",
       "\tX_test.shape=(100, 2)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dataset_list[\"reviews-amazon\"]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRTHCj0pm4A0"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wandb.init(settings=wandb.Settings(init_timeout=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dVaK8droJlcE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'TF-IDF RandomForest' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mkgs210 (mkgs210-itmo-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_202845-j37orvnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/j37orvnt' target=\"_blank\">TF-IDF RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/j37orvnt' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/j37orvnt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'TF-IDF' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 1611)\n",
      "X_test.shape = (100, 1611)\n",
      "Training 'RandomForest' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.73</td></tr><tr><td>Balanced Accuracy</td><td>0.73</td></tr><tr><td>F1 Score (Macro)</td><td>0.72997</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.72727</td></tr><tr><td>Precision (Macro)</td><td>0.73009</td></tr><tr><td>Precision (Positive Class)</td><td>0.73469</td></tr><tr><td>Recall (Macro)</td><td>0.73</td></tr><tr><td>Recall (Positive Class)</td><td>0.72</td></tr><tr><td>Time Classification</td><td>0.15457</td></tr><tr><td>Time Total</td><td>0.95807</td></tr><tr><td>Time Transformation</td><td>0.8035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TF-IDF RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/j37orvnt' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/j37orvnt</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_202845-j37orvnt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'TF-IDF LogisticRegression' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_202857-v58gtoad</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/v58gtoad' target=\"_blank\">TF-IDF LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/v58gtoad' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/v58gtoad</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'TF-IDF' ...\n",
      "X_train.shape = (100, 1611)\n",
      "X_test.shape = (100, 1611)\n",
      "Training 'LogisticRegression' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.77</td></tr><tr><td>Balanced Accuracy</td><td>0.77</td></tr><tr><td>F1 Score (Macro)</td><td>0.76979</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.7767</td></tr><tr><td>Precision (Macro)</td><td>0.77098</td></tr><tr><td>Precision (Positive Class)</td><td>0.75472</td></tr><tr><td>Recall (Macro)</td><td>0.77</td></tr><tr><td>Recall (Positive Class)</td><td>0.8</td></tr><tr><td>Time Classification</td><td>0.20082</td></tr><tr><td>Time Total</td><td>1.00433</td></tr><tr><td>Time Transformation</td><td>0.8035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TF-IDF LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/v58gtoad' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/v58gtoad</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_202857-v58gtoad\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"TF-IDF\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MiK2EYF2EXw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_tP5XpR6Jo0z",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Embeddings RandomForest' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_202908-d2ycuo8u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/d2ycuo8u' target=\"_blank\">Embeddings RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/d2ycuo8u' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/d2ycuo8u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'Embeddings' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64dc714384c428fadf96f1a690b93dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings (intfloat/multilingual-e5-large):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69639e953a4c479786f9bedd0b91ca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings (intfloat/multilingual-e5-large):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 1024)\n",
      "X_test.shape = (100, 1024)\n",
      "Training 'RandomForest' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.97</td></tr><tr><td>Balanced Accuracy</td><td>0.97</td></tr><tr><td>F1 Score (Macro)</td><td>0.97</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.9703</td></tr><tr><td>Precision (Macro)</td><td>0.97019</td></tr><tr><td>Precision (Positive Class)</td><td>0.96078</td></tr><tr><td>Recall (Macro)</td><td>0.97</td></tr><tr><td>Recall (Positive Class)</td><td>0.98</td></tr><tr><td>Time Classification</td><td>0.12788</td></tr><tr><td>Time Total</td><td>8.62585</td></tr><tr><td>Time Transformation</td><td>8.49797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Embeddings RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/d2ycuo8u' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/d2ycuo8u</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_202908-d2ycuo8u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Embeddings LogisticRegression' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_202926-y5drl35d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/y5drl35d' target=\"_blank\">Embeddings LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/y5drl35d' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/y5drl35d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'Embeddings' ...\n",
      "X_train.shape = (100, 1024)\n",
      "X_test.shape = (100, 1024)\n",
      "Training 'LogisticRegression' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.98</td></tr><tr><td>Balanced Accuracy</td><td>0.98</td></tr><tr><td>F1 Score (Macro)</td><td>0.97999</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.97959</td></tr><tr><td>Precision (Macro)</td><td>0.98077</td></tr><tr><td>Precision (Positive Class)</td><td>1</td></tr><tr><td>Recall (Macro)</td><td>0.98</td></tr><tr><td>Recall (Positive Class)</td><td>0.96</td></tr><tr><td>Time Classification</td><td>0.01984</td></tr><tr><td>Time Total</td><td>8.51781</td></tr><tr><td>Time Transformation</td><td>8.49797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Embeddings LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/y5drl35d' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/y5drl35d</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_202926-y5drl35d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"Embeddings\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAb2W0GY3pL9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPT-3.5 Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mY9aMyhzJzV-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Raw Text qwen' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_202936-1du47vrl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/1du47vrl' target=\"_blank\">Raw Text qwen</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/1du47vrl' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/1du47vrl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'Raw Text' ...\n",
      "X_train.shape = (100,)\n",
      "X_test.shape = (100,)\n",
      "Training 'qwen' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'qwen' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.98</td></tr><tr><td>Balanced Accuracy</td><td>0.98</td></tr><tr><td>F1 Score (Macro)</td><td>0.97999</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.97959</td></tr><tr><td>Precision (Macro)</td><td>0.98077</td></tr><tr><td>Precision (Positive Class)</td><td>1</td></tr><tr><td>Recall (Macro)</td><td>0.98</td></tr><tr><td>Recall (Positive Class)</td><td>0.96</td></tr><tr><td>Time Classification</td><td>15.71331</td></tr><tr><td>Time Total</td><td>15.71486</td></tr><tr><td>Time Transformation</td><td>0.00155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Raw Text qwen</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/1du47vrl' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/1du47vrl</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_202936-1du47vrl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"Raw Text\", \"qwen\", d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbMrSGdqt3-8",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPT-4 Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Kw07r5BHJyyH"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"Raw Text\", \"GPT-4\", d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfWTZa8TJvZg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-3.5 (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jC1FuoGwJvZy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Numerical) RandomForest' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_202957-9r53souv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/9r53souv' target=\"_blank\">FELIX qwen (Numerical) RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/9r53souv' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/9r53souv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'FELIX qwen (Numerical)' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff73c7b5b9b4cdaa8a81e88037f5f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/50 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated to 13 features (excl. noise)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669a784f6bdf47d2a01f9b8f7c5c3ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1370b1a11e6462b8acb07fed95a7523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 13)\n",
      "X_test.shape = (100, 13)\n",
      "Imputing missing values in numerical data ...\n",
      "Training 'RandomForest' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.66667</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0.5</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>1</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.05983</td></tr><tr><td>Time Total</td><td>0.11016</td></tr><tr><td>Time Transformation</td><td>0.05032</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Numerical) RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/9r53souv' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/9r53souv</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_202957-9r53souv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Numerical) LogisticRegression' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203003-8i1idz6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/8i1idz6s' target=\"_blank\">FELIX qwen (Numerical) LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/8i1idz6s' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/8i1idz6s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'FELIX qwen (Numerical)' ...\n",
      "X_train.shape = (100, 13)\n",
      "X_test.shape = (100, 13)\n",
      "Imputing missing values in numerical data ...\n",
      "Training 'LogisticRegression' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00425</td></tr><tr><td>Time Total</td><td>0.05457</td></tr><tr><td>Time Transformation</td><td>0.05032</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Numerical) LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/8i1idz6s' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/8i1idz6s</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203003-8i1idz6s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"FELIX qwen (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ2d6PgwJvZy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-3.5 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dpfaoaXsJvZy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Categorical) RandomForest' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203009-xvpljkl3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/xvpljkl3' target=\"_blank\">FELIX qwen (Categorical) RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/xvpljkl3' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/xvpljkl3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'FELIX qwen (Categorical)' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2758b4c344d4eab8061a4759ed1f7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/50 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated to 8 features (excl. noise)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b19130a7bac40cdb83858f1e5d500ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777a366cb4214d87b9e360dd3fe41839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 8)\n",
      "X_test.shape = (100, 8)\n",
      "Creating one-hot encoding of categorical data ...\n",
      "Training 'RandomForest' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.66667</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0.5</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>1</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.06899</td></tr><tr><td>Time Total</td><td>0.12754</td></tr><tr><td>Time Transformation</td><td>0.05854</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Categorical) RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/xvpljkl3' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/xvpljkl3</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203009-xvpljkl3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Categorical) LogisticRegression' on 'amazon_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203015-oespg4h4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/oespg4h4' target=\"_blank\">FELIX qwen (Categorical) LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/oespg4h4' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/oespg4h4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'FELIX qwen (Categorical)' ...\n",
      "X_train.shape = (100, 8)\n",
      "X_test.shape = (100, 8)\n",
      "Creating one-hot encoding of categorical data ...\n",
      "Training 'LogisticRegression' on 'amazon_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00586</td></tr><tr><td>Time Total</td><td>0.0644</td></tr><tr><td>Time Transformation</td><td>0.05854</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Categorical) LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/oespg4h4' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/oespg4h4</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203015-oespg4h4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"FELIX qwen (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiStGdKtJvZz",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-4 (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oTOypqk9JvZz"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"FELIX GPT-4 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S9dIUpqJvZz",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-4 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GqpNApcTJvZz"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"FELIX GPT-4 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zceU5zcjKOTU"
   },
   "source": [
    "## Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4AST-ZiBMW9o"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"Raw Text\", \"RoBERTA-Base 100 Epochs\", d, seed + i)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EyDb-RF5RXJ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment 2: Sample Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-CmyOa_tHl-v",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='cardiffnlp/tweet_sentiment_multilingual'\n",
       "\tshort_name='sentiment'\n",
       "\tcontext_train='Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.'\n",
       "\tcontext_test='Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.'\n",
       "\tclasses=['NEGATIVE', 'POSITIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(10,)\n",
       "\tX_test.shape=(100,)\n",
       "\ty_train.shape=(10,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dataset_list[\"sentiment\"]\n",
    "n_train = 10\n",
    "\n",
    "d = Dataset(\n",
    "    id=d.id,\n",
    "    short_name=d.short_name,\n",
    "    X_train=d.X_train.head(n_train),\n",
    "    X_test=d.X_test,\n",
    "    y_train=d.y_train.head(n_train),\n",
    "    y_test=d.y_test,\n",
    "    pos_class=d.pos_class,\n",
    "    context_train=d.context_train,\n",
    "    context_test=d.context_test,\n",
    "    train_zero_shot=False\n",
    ")\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjMrXOviI7Pe",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nZAZ5WP3I9HU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'TF-IDF RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203021-vbe74bmm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/vbe74bmm' target=\"_blank\">TF-IDF RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/vbe74bmm' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/vbe74bmm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'TF-IDF' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (10, 113)\n",
      "X_test.shape = (100, 113)\n",
      "Training 'RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.34211</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.26</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Time Classification</td><td>0.06776</td></tr><tr><td>Time Total</td><td>0.09263</td></tr><tr><td>Time Transformation</td><td>0.02487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TF-IDF RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/vbe74bmm' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/vbe74bmm</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203021-vbe74bmm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'TF-IDF LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203027-aj6z1qqu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/aj6z1qqu' target=\"_blank\">TF-IDF LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/aj6z1qqu' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/aj6z1qqu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'TF-IDF' ...\n",
      "X_train.shape = (10, 113)\n",
      "X_test.shape = (100, 113)\n",
      "Training 'LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.34211</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.26</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00939</td></tr><tr><td>Time Total</td><td>0.03427</td></tr><tr><td>Time Transformation</td><td>0.02487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TF-IDF LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/aj6z1qqu' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/aj6z1qqu</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203027-aj6z1qqu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"TF-IDF\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbRgFopLJp3n",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Sl_gHTjsHvoB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Embeddings RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203032-25e326d7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/25e326d7' target=\"_blank\">Embeddings RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/25e326d7' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/25e326d7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'Embeddings' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1559661d8b18417eafa876ef5698ac49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings (intfloat/multilingual-e5-large):   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ff535514ba4859915621100fd1fbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings (intfloat/multilingual-e5-large):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (10, 1024)\n",
      "X_test.shape = (100, 1024)\n",
      "Training 'RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.54</td></tr><tr><td>Balanced Accuracy</td><td>0.52083</td></tr><tr><td>F1 Score (Macro)</td><td>0.38667</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.08</td></tr><tr><td>Precision (Macro)</td><td>0.76531</td></tr><tr><td>Precision (Positive Class)</td><td>1</td></tr><tr><td>Recall (Macro)</td><td>0.52083</td></tr><tr><td>Recall (Positive Class)</td><td>0.04167</td></tr><tr><td>Time Classification</td><td>0.06507</td></tr><tr><td>Time Total</td><td>4.35025</td></tr><tr><td>Time Transformation</td><td>4.28518</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Embeddings RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/25e326d7' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/25e326d7</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203032-25e326d7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Embeddings LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_203044-2r6doazx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/2r6doazx' target=\"_blank\">Embeddings LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/2r6doazx' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/2r6doazx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'Embeddings' ...\n",
      "X_train.shape = (10, 1024)\n",
      "X_test.shape = (100, 1024)\n",
      "Training 'LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.59</td></tr><tr><td>Balanced Accuracy</td><td>0.57532</td></tr><tr><td>F1 Score (Macro)</td><td>0.51645</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.32787</td></tr><tr><td>Precision (Macro)</td><td>0.66622</td></tr><tr><td>Precision (Positive Class)</td><td>0.76923</td></tr><tr><td>Recall (Macro)</td><td>0.57532</td></tr><tr><td>Recall (Positive Class)</td><td>0.20833</td></tr><tr><td>Time Classification</td><td>0.01598</td></tr><tr><td>Time Total</td><td>4.30116</td></tr><tr><td>Time Transformation</td><td>4.28518</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Embeddings LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/2r6doazx' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/2r6doazx</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203044-2r6doazx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"Embeddings\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsCcfPIaJze9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-3.5 (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "R1wKRIdJKEN7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Numerical) RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n",
      "Transforming data using 'FELIX qwen (Numerical)' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c949ffa866e248669a5bd8583da7a26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/7 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features identified as noise. Treating them as one cluster\n",
      "Consolidated to 1 features (excl. noise)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d894d538cfd842319cae2687ad72afda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/10 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e98304e1f544b185fd61bb5d905fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (10, 2)\n",
      "X_test.shape = (100, 2)\n",
      "Imputing missing values in numerical data ...\n",
      "Training 'RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.34211</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.26</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.06419</td></tr><tr><td>Time Total</td><td>0.09002</td></tr><tr><td>Time Transformation</td><td>0.02583</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX GPT-3.5 (Numerical) RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/x3d5sg34' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/x3d5sg34</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_203052-x3d5sg34\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Numerical) LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213621-9lua25lx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/9lua25lx' target=\"_blank\">FELIX qwen (Numerical) LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/9lua25lx' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/9lua25lx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'FELIX qwen (Numerical)' ...\n",
      "X_train.shape = (10, 2)\n",
      "X_test.shape = (100, 2)\n",
      "Imputing missing values in numerical data ...\n",
      "Training 'LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.34211</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.26</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00544</td></tr><tr><td>Time Total</td><td>0.03127</td></tr><tr><td>Time Transformation</td><td>0.02583</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Numerical) LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/9lua25lx' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/9lua25lx</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213621-9lua25lx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"FELIX qwen (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYxoWRAnJ69Y",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-3.5 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NADeCdscKLy1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Categorical) RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213628-x5ten0sj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/x5ten0sj' target=\"_blank\">FELIX qwen (Categorical) RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/x5ten0sj' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/x5ten0sj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'FELIX qwen (Categorical)' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a38b23529a941f795c34e91c83244aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/7 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated to 2 features (excl. noise)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afdc2dfc0ad49a6a30404981c2943a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/10 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f902ef8c44d729019ca6d91183b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (10, 3)\n",
      "X_test.shape = (100, 3)\n",
      "Creating one-hot encoding of categorical data ...\n",
      "Training 'RandomForest' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.34211</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.26</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.06714</td></tr><tr><td>Time Total</td><td>0.09221</td></tr><tr><td>Time Transformation</td><td>0.02507</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Categorical) RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/x5ten0sj' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/x5ten0sj</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213628-x5ten0sj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Categorical) LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213634-qmu9vylu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/qmu9vylu' target=\"_blank\">FELIX qwen (Categorical) LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/qmu9vylu' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/qmu9vylu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'FELIX qwen (Categorical)' ...\n",
      "X_train.shape = (10, 3)\n",
      "X_test.shape = (100, 3)\n",
      "Creating one-hot encoding of categorical data ...\n",
      "Training 'LogisticRegression' on 'cardiffnlp/tweet_sentiment_multilingual' training set ...\n",
      "Predicting class labels of 'cardiffnlp/tweet_sentiment_multilingual' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.52</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.34211</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.26</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00399</td></tr><tr><td>Time Total</td><td>0.02906</td></tr><tr><td>Time Transformation</td><td>0.02507</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Categorical) LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/qmu9vylu' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/qmu9vylu</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213634-qmu9vylu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"FELIX qwen (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQsICM76J8_i",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-4 (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VCh8Lh2IKOYA"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"FELIX GPT-4 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v30AJkd0J-Kk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FELIX GPT-4 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddlZUt3-JHAW"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"FELIX GPT-4 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftecAjQuOmIG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKMBVZadOoEC"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"Raw Text\", \"RoBERTA-Base 100 Epochs\", d_sample, seed + i)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rax2gybu5Y2M",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment 3: Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ils-5hqp5cmB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='amazon_polarity>yelp_polarity'\n",
       "\tshort_name='reviews-amazon>reviews-yelp'\n",
       "\tcontext_train='Examples are product reviews from Amazon that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tcontext_test='Examples are reviews from Yelp that are either 'NEGATIVE' (rating with 1 or 2 stars) or 'POSITIVE' (rating with 4 or 5 stars). The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' reviews.'\n",
       "\tclasses=['POSITIVE', 'NEGATIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100, 2)\n",
       "\tX_test.shape=(100, 1)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_name = \"reviews-amazon\"\n",
    "test_name = \"reviews-yelp\"\n",
    "\n",
    "d_train = dataset_list[train_name]\n",
    "d_test = dataset_list[test_name]\n",
    "\n",
    "if d_train.pos_class != d_test.pos_class:\n",
    "    print(\"Warning: Train and test datasets have different classes. Do you still want to continue?\\n\")\n",
    "\n",
    "d = Dataset(\n",
    "    id=f\"{d_train.id}>{d_test.id}\",\n",
    "    short_name=f\"{d_train.short_name}>{d_test.short_name}\",\n",
    "    X_train=d_train.X_train,\n",
    "    X_test=d_test.X_test,\n",
    "    y_train=d_train.y_train,\n",
    "    y_test=d_test.y_test,\n",
    "    pos_class=d_train.pos_class,\n",
    "    context_train=d_train.context_train,\n",
    "    context_test=d_test.context_test,\n",
    "    train_zero_shot=False\n",
    ")\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmpScVWCS7Vh"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "P6GqeGO2S1sI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'TF-IDF RandomForest' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213653-mug69g7m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/mug69g7m' target=\"_blank\">TF-IDF RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/mug69g7m' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/mug69g7m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'TF-IDF' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 1611)\n",
      "X_test.shape = (100, 1611)\n",
      "Training 'RandomForest' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.61</td></tr><tr><td>Balanced Accuracy</td><td>0.61</td></tr><tr><td>F1 Score (Macro)</td><td>0.58822</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.68293</td></tr><tr><td>Precision (Macro)</td><td>0.63952</td></tr><tr><td>Precision (Positive Class)</td><td>0.57534</td></tr><tr><td>Recall (Macro)</td><td>0.61</td></tr><tr><td>Recall (Positive Class)</td><td>0.84</td></tr><tr><td>Time Classification</td><td>0.1568</td></tr><tr><td>Time Total</td><td>1.18015</td></tr><tr><td>Time Transformation</td><td>1.02335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TF-IDF RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/mug69g7m' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/mug69g7m</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213653-mug69g7m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'TF-IDF LogisticRegression' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213747-o97p9b2g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/o97p9b2g' target=\"_blank\">TF-IDF LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/o97p9b2g' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/o97p9b2g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'TF-IDF' ...\n",
      "X_train.shape = (100, 1611)\n",
      "X_test.shape = (100, 1611)\n",
      "Training 'LogisticRegression' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.62</td></tr><tr><td>Balanced Accuracy</td><td>0.62</td></tr><tr><td>F1 Score (Macro)</td><td>0.6124</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.66667</td></tr><tr><td>Precision (Macro)</td><td>0.63021</td></tr><tr><td>Precision (Positive Class)</td><td>0.59375</td></tr><tr><td>Recall (Macro)</td><td>0.62</td></tr><tr><td>Recall (Positive Class)</td><td>0.76</td></tr><tr><td>Time Classification</td><td>0.27674</td></tr><tr><td>Time Total</td><td>1.30009</td></tr><tr><td>Time Transformation</td><td>1.02335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TF-IDF LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/o97p9b2g' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/o97p9b2g</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213747-o97p9b2g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"TF-IDF\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEEZW0LTVmc"
   },
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zw9AmXp7TVmk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Embeddings RandomForest' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213813-5huzzvk7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/5huzzvk7' target=\"_blank\">Embeddings RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/5huzzvk7' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/5huzzvk7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'Embeddings' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c6a3a5c8c041f0b7bf761bf8407a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings (intfloat/multilingual-e5-large):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce2beacd8ae4d63aae77b18c03658af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings (intfloat/multilingual-e5-large):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 1024)\n",
      "X_test.shape = (100, 1024)\n",
      "Training 'RandomForest' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.93</td></tr><tr><td>Balanced Accuracy</td><td>0.93</td></tr><tr><td>F1 Score (Macro)</td><td>0.92994</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.93204</td></tr><tr><td>Precision (Macro)</td><td>0.93155</td></tr><tr><td>Precision (Positive Class)</td><td>0.90566</td></tr><tr><td>Recall (Macro)</td><td>0.93</td></tr><tr><td>Recall (Positive Class)</td><td>0.96</td></tr><tr><td>Time Classification</td><td>0.12938</td></tr><tr><td>Time Total</td><td>5.62617</td></tr><tr><td>Time Transformation</td><td>5.49678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Embeddings RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/5huzzvk7' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/5huzzvk7</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213813-5huzzvk7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'Embeddings LogisticRegression' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213853-1sa52xjd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/1sa52xjd' target=\"_blank\">Embeddings LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/1sa52xjd' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/1sa52xjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'Embeddings' ...\n",
      "X_train.shape = (100, 1024)\n",
      "X_test.shape = (100, 1024)\n",
      "Training 'LogisticRegression' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.95</td></tr><tr><td>Balanced Accuracy</td><td>0.95</td></tr><tr><td>F1 Score (Macro)</td><td>0.94995</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.95146</td></tr><tr><td>Precision (Macro)</td><td>0.95163</td></tr><tr><td>Precision (Positive Class)</td><td>0.92453</td></tr><tr><td>Recall (Macro)</td><td>0.95</td></tr><tr><td>Recall (Positive Class)</td><td>0.98</td></tr><tr><td>Time Classification</td><td>0.01627</td></tr><tr><td>Time Total</td><td>5.51305</td></tr><tr><td>Time Transformation</td><td>5.49678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Embeddings LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/1sa52xjd' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/1sa52xjd</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213853-1sa52xjd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"Embeddings\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKCfr7hrTVml"
   },
   "source": [
    "## FELIX GPT-3.5 (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KDXgj0rpTVml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Numerical) RandomForest' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_213952-0z5b0w32</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/0z5b0w32' target=\"_blank\">FELIX qwen (Numerical) RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/0z5b0w32' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/0z5b0w32</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'FELIX qwen (Numerical)' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0ba3f561fb4011bc49764f570168d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/50 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated to 9 features (excl. noise)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b96a6b3e63b4d0d92cbf25a2571f9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be4fcae5a214adc924d20c3de5c44ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 9)\n",
      "X_test.shape = (100, 9)\n",
      "Imputing missing values in numerical data ...\n",
      "Training 'RandomForest' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.66667</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0.5</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>1</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.06139</td></tr><tr><td>Time Total</td><td>0.10436</td></tr><tr><td>Time Transformation</td><td>0.04297</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Numerical) RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/0z5b0w32' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/0z5b0w32</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_213952-0z5b0w32\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Numerical) LogisticRegression' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_214002-uhazicsl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/uhazicsl' target=\"_blank\">FELIX qwen (Numerical) LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/uhazicsl' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/uhazicsl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'FELIX qwen (Numerical)' ...\n",
      "X_train.shape = (100, 9)\n",
      "X_test.shape = (100, 9)\n",
      "Imputing missing values in numerical data ...\n",
      "Training 'LogisticRegression' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00307</td></tr><tr><td>Time Total</td><td>0.04604</td></tr><tr><td>Time Transformation</td><td>0.04297</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Numerical) LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/uhazicsl' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/uhazicsl</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_214002-uhazicsl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"FELIX qwen (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lxVRAcdTVml"
   },
   "source": [
    "## FELIX GPT-3.5 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "I15YVO_wTVml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Categorical) RandomForest' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e357fe49813d4e2fafe5239704e6a114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_214010-gyzrb02j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/gyzrb02j' target=\"_blank\">FELIX qwen (Categorical) RandomForest</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/gyzrb02j' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/gyzrb02j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data using 'FELIX qwen (Categorical)' ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1176d2e757646d28ab2abed3689847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/50 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated to 9 features (excl. noise)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0bd951405c441c9b3e7d238df0b8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f679bec2e9bd402eadf225697b39e2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring examples:   0%|          | 0/100 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (100, 9)\n",
      "X_test.shape = (100, 9)\n",
      "Creating one-hot encoding of categorical data ...\n",
      "Training 'RandomForest' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'RandomForest' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0.66667</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0.5</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>1</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.05955</td></tr><tr><td>Time Total</td><td>0.10185</td></tr><tr><td>Time Transformation</td><td>0.0423</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Categorical) RandomForest</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/gyzrb02j' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/gyzrb02j</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_214010-gyzrb02j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run 'FELIX qwen (Categorical) LogisticRegression' on 'amazon_polarity>yelp_polarity' ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Maxim\\Desktop\\hw\\X5\\FELIX\\wandb\\run-20250112_214020-k8gw43qy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/k8gw43qy' target=\"_blank\">FELIX qwen (Categorical) LogisticRegression</a></strong> to <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/k8gw43qy' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/k8gw43qy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data transformation 'FELIX qwen (Categorical)' ...\n",
      "X_train.shape = (100, 9)\n",
      "X_test.shape = (100, 9)\n",
      "Creating one-hot encoding of categorical data ...\n",
      "Training 'LogisticRegression' on 'amazon_polarity>yelp_polarity' training set ...\n",
      "Predicting class labels of 'amazon_polarity>yelp_polarity' test set with trained 'LogisticRegression' ...\n",
      "Finishing run and logging the results ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Maxim\\Desktop\\hw\\X5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parameter 'callback' is set but cannot be saved to instance.\n",
      "Warning: Parameters stored in '_hdbscan' cannot be saved to instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁</td></tr><tr><td>Balanced Accuracy</td><td>▁</td></tr><tr><td>F1 Score (Macro)</td><td>▁</td></tr><tr><td>F1 Score (Positive Class)</td><td>▁</td></tr><tr><td>Generation Cost</td><td>▁</td></tr><tr><td>Precision (Macro)</td><td>▁</td></tr><tr><td>Precision (Positive Class)</td><td>▁</td></tr><tr><td>Recall (Macro)</td><td>▁</td></tr><tr><td>Recall (Positive Class)</td><td>▁</td></tr><tr><td>Scoring Cost</td><td>▁</td></tr><tr><td>Time Classification</td><td>▁</td></tr><tr><td>Time Total</td><td>▁</td></tr><tr><td>Time Transformation</td><td>▁</td></tr><tr><td>Total Cost</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.5</td></tr><tr><td>Balanced Accuracy</td><td>0.5</td></tr><tr><td>F1 Score (Macro)</td><td>0.33333</td></tr><tr><td>F1 Score (Positive Class)</td><td>0</td></tr><tr><td>Generation Cost</td><td>0</td></tr><tr><td>Precision (Macro)</td><td>0.25</td></tr><tr><td>Precision (Positive Class)</td><td>0</td></tr><tr><td>Recall (Macro)</td><td>0.5</td></tr><tr><td>Recall (Positive Class)</td><td>0</td></tr><tr><td>Scoring Cost</td><td>0</td></tr><tr><td>Time Classification</td><td>0.00363</td></tr><tr><td>Time Total</td><td>0.04593</td></tr><tr><td>Time Transformation</td><td>0.0423</td></tr><tr><td>Total Cost</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FELIX qwen (Categorical) LogisticRegression</strong> at: <a href='https://wandb.ai/mkgs210-itmo-university/felix/runs/k8gw43qy' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix/runs/k8gw43qy</a><br> View project at: <a href='https://wandb.ai/mkgs210-itmo-university/felix' target=\"_blank\">https://wandb.ai/mkgs210-itmo-university/felix</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250112_214020-k8gw43qy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Run(\"FELIX qwen (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fXd0nhzTVml"
   },
   "source": [
    "## FELIX GPT-4 (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qTBVyVnOTVmm"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"FELIX GPT-4 (Numerical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtC25X2ATVmm"
   },
   "source": [
    "## FELIX GPT-4 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_bxBPrqMTVmm"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"FELIX GPT-4 (Categorical)\", [\"RandomForest\", \"LogisticRegression\"], d, seed)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO6m5GNiPCi2"
   },
   "source": [
    "## Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "AR_6SpsxPEQE"
   },
   "outputs": [],
   "source": [
    "# run = Run(\"Raw Text\", \"RoBERTA-Base 100 Epochs\", d, seed + i)\n",
    "# run.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SLghYGJGVrF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment 5: Internal Validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChrboR85HquD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 5.1: Clustering Validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FLyokbtBx3E",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Learn Numeric Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xNSJksX_cmG3"
   },
   "outputs": [],
   "source": [
    "results_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "hQzzM8mFkUxM"
   },
   "outputs": [],
   "source": [
    "def log_results(dataset_id, method, felix, features, f1_rf, f1_lr, probs_lr, comment=\"\"):\n",
    "    results_log.append({\n",
    "        \"dataset\": dataset_id,\n",
    "        \"method\": method,\n",
    "        \"felix_variant\": f\"FELIX GPT-{'3.5' if '3.5' in felix.llm_scoring else '4'} {'Categorical' if felix.discrete_features else 'Numerical'}\",\n",
    "        \"n_features\": len(features),\n",
    "        \"feature_set\": features,\n",
    "        \"f1_rf\": f1_rf,\n",
    "        \"f1_lr\": f1_lr,\n",
    "        \"probs_lr\": probs_lr,\n",
    "        \"comment\": comment\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "j3OdZZACBQCt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\tid='cardiffnlp/tweet_sentiment_multilingual'\n",
       "\tshort_name='sentiment'\n",
       "\tcontext_train='Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.'\n",
       "\tcontext_test='Examples are tweets in multiple languages which express either a 'NEGATIVE' or 'POSITIVE' sentiment. The downstream machine learning task is learning a classifier (e.g., Random Forest) that learns to distinguish 'NEGATIVE' and 'POSITIVE' tweets, independent of the language they are written in.'\n",
       "\tclasses=['NEGATIVE', 'POSITIVE']\n",
       "\tpos_class='POSITIVE'\n",
       "\ttrain_zero_shot=False\n",
       "\tX_train.shape=(100,)\n",
       "\tX_test.shape=(100,)\n",
       "\ty_train.shape=(100,)\n",
       "\ty_test.shape=(100,)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a dataset\n",
    "d = dataset_list[\"sentiment\"]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RV7c_fNhB2uu"
   },
   "outputs": [],
   "source": [
    "# Configure FELIX\n",
    "felix = FELIX(\n",
    "    context=d.context_train,\n",
    "    temperature_scoring=0.0,\n",
    "    llm_scoring=\"gpt-3.5-turbo-16k\",\n",
    "    discrete_features=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4cIvIemyj7zS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d777a64123e45dfbe5c6988cf772258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating features:   0%|          | 0/17 [00:00<?, ?s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_generation = 30\n",
    "\n",
    "# Fit FELIX to the dataset to learn features\n",
    "felix._full_feature_set = felix.generate_features(d.X_train.head(n_generation), d.y_train.head(n_generation))\n",
    "felix._full_feature_set = felix._ensure_unique_feature_names(felix._full_feature_set)\n",
    "felix._features = felix._full_feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCKXJUJglvu6"
   },
   "source": [
    "Save the learned features for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "mTM_7yhy-GKs"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Serializing json\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import datetime\n",
    "\n",
    "# Serializing json\n",
    "json_object = felix._features.json(indent=4)\n",
    "\n",
    "# Store the results\n",
    "filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Features {d.short_name}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(json_object)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STGoRxof-MBR"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload the features JSON\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Parse the results\n",
    "flat_list = json.loads(uploaded[filename])[\"features\"]\n",
    "if felix.discrete_features:\n",
    "    felix._features = CategoricalFeatureSet(features=flat_list)\n",
    "else:\n",
    "    felix._features = NumericalFeatureSet(features=flat_list)\n",
    "felix._full_feature_set = felix._features\n",
    "len(felix._features.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d90sEUGRya5d",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Score All Learned Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHk9EdaDDTJq"
   },
   "outputs": [],
   "source": [
    "df_scores = felix.transform(d.X_test)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfw3bEF2-j3f"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import datetime\n",
    "\n",
    "# Save the scores data as a CSV\n",
    "filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Scores {d.short_name}.csv\"\n",
    "df_scores.to_csv(filename, index=False)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMH0NElCN8tb"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import datetime\n",
    "\n",
    "# Upload the scores CSV\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the scores\n",
    "df_scores = pd.read_csv(filename)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzTgw82m6KJp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Handle Missing Values (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0R8v9dlCxa4"
   },
   "outputs": [],
   "source": [
    "col_drop_thres = 0.1\n",
    "n_neighbors = 5\n",
    "\n",
    "# Remove all columns that have more than 'col_drop_thres' missing values\n",
    "missing_percentage = df_scores.isnull().mean()                                  # Calculate the percentage of missing values for each column\n",
    "cols_to_drop = missing_percentage[missing_percentage > col_drop_thres].index    # Identify columns that have more than 'col_drop_thres' % missing values\n",
    "df_scores = df_scores.drop(columns=cols_to_drop)                                # Drop the columns from the DataFrame\n",
    "\n",
    "# Estimate missing values with k-Nearest-Neighbors imputations\n",
    "imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"distance\")\n",
    "imputer.set_output(transform=\"pandas\")\n",
    "df_scores = imputer.fit_transform(df_scores)\n",
    "\n",
    "print(\"Columns removed:\", cols_to_drop)\n",
    "print(\"Columns with missing features remaining:\", df_scores.isnull().any().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw8YGtTNDx53"
   },
   "outputs": [],
   "source": [
    "# Remove features from FELIX that could not be scored reliably (i.e., more than 'col_drop_thres' missing values)\n",
    "felix._features.features = [f for f in felix._features.features if f.name not in cols_to_drop]\n",
    "\n",
    "print(len(felix._features.features))\n",
    "print(df_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYp8UxbpDtv4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Handle Missing Values (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl10p0MK6JnC"
   },
   "outputs": [],
   "source": [
    "nan_columns = df_scores.isna().all()[df_scores.isna().all() == True].index.to_list()\n",
    "\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kicLl5cX6xmS"
   },
   "outputs": [],
   "source": [
    "# Remove features from FELIX that could not be scored (i.e., all rows have NaN values)\n",
    "felix._features.features = [f for f in felix._features.features if f.name not in nan_columns]\n",
    "\n",
    "# Remove respective columns from the scores dataset\n",
    "df_scores = df_scores.drop(columns=nan_columns)\n",
    "\n",
    "print(len(felix._features.features))\n",
    "print(df_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT985Mpuk533",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Feature Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78mM1q0Gr-0f"
   },
   "outputs": [],
   "source": [
    "# Create text embeddings for each feature\n",
    "felix._feature_embeddings = felix._create_feature_embeddings(felix._features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iujEVCf2ylaY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare Feature Selection Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xg-zyQzG9fJR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def encode_data(X_train, X_test):\n",
    "    # Drop columns that have no values\n",
    "    nan_columns_train = X_train.isna().all()[X_train.isna().all() == True].index.to_list()\n",
    "    nan_columns_test = X_test.isna().all()[X_test.isna().all() == True].index.to_list()\n",
    "    # nan_columns = nan_columns_train + nan_columns_test\n",
    "    nan_columns = [x for x in nan_columns_train if x in nan_columns_test]\n",
    "    X_train = X_train.drop(columns=nan_columns)\n",
    "    X_test = X_test.drop(columns=nan_columns)\n",
    "\n",
    "    # Identify categorical columns (assuming the same columns in train and test)\n",
    "    categorical_columns = X_train.select_dtypes(include=[object]).columns\n",
    "    non_categorical_columns = X_train.select_dtypes(exclude=[object]).columns\n",
    "\n",
    "    # Encode values based on column type\n",
    "    if len(categorical_columns) > 0 and len(non_categorical_columns) == 0:\n",
    "        data_transformer = DataTransformer(dataset=None)\n",
    "        return data_transformer.one_hot_encode(X_train, X_test)\n",
    "    elif len(non_categorical_columns) > 0 and len(categorical_columns) == 0:\n",
    "        return X_train, X_test\n",
    "    else:\n",
    "        print(\"Categorical columns =\", categorical_columns)\n",
    "        print(\"Numerical columns =\", non_categorical_columns)\n",
    "        print(\"NaN columns training =\", nan_columns_train)\n",
    "        print(\"NaN columns test =\", nan_columns_test)\n",
    "        print(\"NaN columns =\", nan_columns)\n",
    "        raise ValueError(\"Cannot handle mixed dataset of categorical and numerical columns.\")\n",
    "\n",
    "\n",
    "def evaluate_feature_set(feature_set, X, y, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True):\n",
    "    # Define classifier models\n",
    "    if lr_probs or lr_f1:\n",
    "        lr_model = LogisticRegression(max_iter=10000, random_state=random_state)\n",
    "    if rf_f1:\n",
    "        rf_model = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "    # Create lists to store the scores from each cross-validation fold\n",
    "    avg_probs_lr = []\n",
    "    avg_scores_lr = []\n",
    "    avg_scores_rf = []\n",
    "\n",
    "    # Evaluate performance using k-fold cross validation\n",
    "    kf = KFold(n_splits=cv_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Create train-test split\n",
    "        X_train = X.iloc[train_index][feature_set]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index][feature_set]\n",
    "        y_test = y.iloc[test_index]\n",
    "\n",
    "        # Convert columns that contain categorical data into a one-hot representation\n",
    "        X_train, X_test = encode_data(X_train, X_test)\n",
    "\n",
    "        # Train the models with the feature set\n",
    "        if lr_probs or lr_f1:\n",
    "            lr_model.fit(X_train, y_train)\n",
    "        if rf_f1:\n",
    "            rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the trained models on the test set\n",
    "        if lr_probs:\n",
    "            probs_lr = lr_model.predict_proba(X_test)\n",
    "            avg_prob = np.mean([p[lr_model.classes_.tolist().index(c)] for p, c in zip(probs_lr, y_test)]) # Calculate the average predicted probability of belonging to the ground truth class\n",
    "            avg_probs_lr.append(avg_prob)\n",
    "        else:\n",
    "            avg_probs_lr.append(0.0)\n",
    "        if lr_f1:\n",
    "            preds_lr = lr_model.predict(X_test)\n",
    "            f1_lr = f1_score(y_test, preds_lr, average=\"macro\")\n",
    "            avg_scores_lr.append(f1_lr)\n",
    "        else:\n",
    "            avg_scores_lr.append(0.0)\n",
    "        if rf_f1:\n",
    "            preds_rf = rf_model.predict(X_test)\n",
    "            f1_rf = f1_score(y_test, preds_rf, average=\"macro\")\n",
    "            avg_scores_rf.append(f1_rf)\n",
    "        else:\n",
    "            avg_scores_rf.append(0.0)\n",
    "\n",
    "    # Calculate and return average scores across all folds\n",
    "    return np.mean(avg_probs_lr), np.mean(avg_scores_lr), np.mean(avg_scores_rf)\n",
    "\n",
    "\n",
    "def forward_selection(X, y, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True):\n",
    "    # Create lists to store the features selected (in order of forward selection) and corresponding scores\n",
    "    feature_set = []\n",
    "    scores = []\n",
    "\n",
    "    # Iteratively add the feature that leads to the best classification performance\n",
    "    while len(feature_set) < X.shape[1]:\n",
    "        # Keep track of the best performance so far\n",
    "        best_feature = None\n",
    "        best_score = -np.inf\n",
    "        prob_lr = -np.inf\n",
    "        score_lr = -np.inf\n",
    "        score_rf = -np.inf\n",
    "\n",
    "        # Iterate over all features that are not yet in the feature set and find the one that leads to the best performance\n",
    "        for feature in X.columns:\n",
    "            if feature not in feature_set:\n",
    "                # Add the current feature to the set\n",
    "                features_to_use = feature_set + [feature]\n",
    "\n",
    "                # Evaluate the classification performance using this feature set\n",
    "                lr_prob, lr_score, rf_score = evaluate_feature_set(features_to_use, X, y, cv_splits, random_state, lr_probs=lr_probs, lr_f1=lr_f1, rf_f1=rf_f1)\n",
    "\n",
    "                # Calculate the final decision criterion\n",
    "                if lr_f1 and rf_f1:\n",
    "                    score = np.mean([lr_score, rf_score])\n",
    "                elif lr_f1:\n",
    "                    score = lr_score\n",
    "                elif rf_f1:\n",
    "                    score = rf_score\n",
    "                elif lr_probs:\n",
    "                    score = lr_prob\n",
    "\n",
    "                # Check if this is the best score\n",
    "                if score > best_score:\n",
    "                    best_feature = feature\n",
    "                    best_score = score\n",
    "\n",
    "                    # Also store the other scores as reference\n",
    "                    prob_lr = lr_prob\n",
    "                    score_lr = lr_score\n",
    "                    score_rf = rf_score\n",
    "\n",
    "        # Add best feature to our feature set\n",
    "        feature_set.append(best_feature)\n",
    "        scores.append(best_score)\n",
    "\n",
    "        log_results(d.id, \"Supervised Forward Selection\", felix, feature_set, score_rf, score_lr, prob_lr, f\"Added feature {best_feature}\")\n",
    "\n",
    "        print(f\"n_features = {len(feature_set)}. Score = {best_score}. LR_prob = {prob_lr}. LR_F1 = {score_lr}. RF_F1 = {score_rf}. Added feature {best_feature}\")\n",
    "\n",
    "    return feature_set, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeZKGOG7n1QW",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Supervised Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8gWzPnlFIRg"
   },
   "outputs": [],
   "source": [
    "features_forward, scores_forward = forward_selection(df_scores, d.y_test, cv_splits=5, random_state=seed, lr_probs=True, lr_f1=True, rf_f1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vR32mZMiAVtj"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "df_forward = pd.DataFrame({\"Feature\": features_forward, \"F1 (LR-RF avg.)\": scores_forward})\n",
    "\n",
    "filename = \"2023_11_02 - FELIX Forward Selection Fake News.csv\"\n",
    "df_forward.to_csv(filename, index=False)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yIMXSEgRlAz"
   },
   "outputs": [],
   "source": [
    "# Upload the results\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the scores\n",
    "df_forward = pd.read_csv(filename)\n",
    "\n",
    "features_forward = df_forward[\"Feature\"].tolist()\n",
    "scores_forward = df_forward[\"F1 (LR-RF avg.)\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54KJ85svoAB2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rL0431PxLPlG"
   },
   "outputs": [],
   "source": [
    "def run_hdbscan(feature_embeddings, df_scores, d, keep_noise=True, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0, verbose=False):\n",
    "    # Cluster the features with HDBSCAN\n",
    "    np.random.seed(0)\n",
    "    hdbscan = HDBSCAN(min_cluster_size=2, allow_single_cluster=True, cluster_selection_method=cluster_selection_method, cluster_selection_epsilon=cluster_selection_epsilon, alpha=alpha)\n",
    "    cluster_labels = hdbscan.fit_predict(feature_embeddings)\n",
    "\n",
    "    if keep_noise:\n",
    "        # Assign all features identified as noise (i.e., not part of a cluster) to their own cluster\n",
    "        next_label = np.max(cluster_labels) + 1\n",
    "        for i in range(len(cluster_labels)):\n",
    "            if cluster_labels[i] == -1:\n",
    "                cluster_labels[i] = next_label\n",
    "                next_label += 1\n",
    "\n",
    "        # Select the most representative feature from each cluster\n",
    "        features_hdbscan = felix._select_representative_features(felix._features, felix._feature_embeddings, cluster_labels)\n",
    "        if verbose:\n",
    "            print(f\"HDBSCAN selected {len(features_hdbscan.features)} features (including noise)\")\n",
    "    else:\n",
    "        # If all features have been identified as noise, put them all in one cluster\n",
    "        if (np.array(cluster_labels) == -1).all():\n",
    "            cluster_labels = [0 for _ in cluster_labels]\n",
    "            if verbose:\n",
    "                print(\"All features identified as noise. Treating them as one cluster\")\n",
    "\n",
    "        # Prune the feature set to only those features that are not identified as noise\n",
    "        pruned_feature_set = felix._features.copy(deep=True)\n",
    "        pruned_feature_set.features = [f for f, l in zip(felix._features.features, cluster_labels) if l != -1]\n",
    "        pruned_embeddings = np.array([e for e, l in zip(felix._feature_embeddings, cluster_labels) if l != -1])\n",
    "        pruned_labels = [l for l in cluster_labels if l != -1]\n",
    "\n",
    "        # Select the most representative feature from each cluster (excluding noise)\n",
    "        features_hdbscan = felix._select_representative_features(pruned_feature_set, pruned_embeddings, pruned_labels)\n",
    "        if verbose:\n",
    "            print(f\"HDBSCAN selected {len(features_hdbscan.features)} features (without noise)\")\n",
    "\n",
    "\n",
    "    lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_hdbscan.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n",
    "    score_hdbscan = np.mean([lr_score, rf_score])\n",
    "\n",
    "    return features_hdbscan, rf_score, lr_score, lr_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9EpBU2ujs8k"
   },
   "outputs": [],
   "source": [
    "features_hdbscan, rf_score, lr_score, lr_prob = run_hdbscan(felix._feature_embeddings, df_scores, d, keep_noise=True, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0)\n",
    "log_results(d.id, \"HDBSCAN (w/ noise)\", felix, [f.name for f in features_hdbscan.features], rf_score, lr_score, lr_prob, comment=\"keep_noise=True, cluster_selection_method='leaf', cluster_selection_epsilon=0.0, alpha=1.0\")\n",
    "print(f\"F1 = {np.mean([rf_score, lr_score])}. n_features = {len(features_hdbscan.features)}\")\n",
    "\n",
    "features_hdbscan, rf_score, lr_score, lr_prob = run_hdbscan(felix._feature_embeddings, df_scores, d, keep_noise=False, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.0, alpha=1.0)\n",
    "log_results(d.id, \"HDBSCAN (w/o noise)\", felix, [f.name for f in features_hdbscan.features], rf_score, lr_score, lr_prob, comment=\"keep_noise=False, cluster_selection_method='leaf', cluster_selection_epsilon=0.0, alpha=1.0\")\n",
    "print(f\"F1 = {np.mean([rf_score, lr_score])}. n_features = {len(features_hdbscan.features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY1n8-8jlt1M"
   },
   "source": [
    "Perform a grid search for the optimal HDBSCAN hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRzImoSFWvif"
   },
   "outputs": [],
   "source": [
    "options_keep_noise = [True, False]\n",
    "options_cluster_selection_method = [\"leaf\", \"eom\"]\n",
    "options_cluster_selection_epsilon = [float(x) / 10 for x in range(0, 11, 2)]\n",
    "options_alpha = [float(x) / 10 for x in range(5, 16, 1)]\n",
    "\n",
    "tuning_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vN0pslcQHi_"
   },
   "outputs": [],
   "source": [
    "for kn in options_keep_noise:\n",
    "    for csm in options_cluster_selection_method:\n",
    "        for epsilon in options_cluster_selection_epsilon:\n",
    "            for alpha in options_alpha:\n",
    "                features_hdbscan, rf_score, lr_score, lr_prob = run_hdbscan(felix._feature_embeddings, df_scores, d, keep_noise=kn, cluster_selection_method=csm, cluster_selection_epsilon=epsilon, alpha=alpha)\n",
    "                r = {\n",
    "                    \"Dataset\": d.id,\n",
    "                    \"keep_noise\": kn,\n",
    "                    \"cluster_selection_method\": csm,\n",
    "                    \"epsilon\": epsilon,\n",
    "                    \"alpha\": alpha,\n",
    "                    \"F1 Average\": np.mean([rf_score, lr_score]),\n",
    "                    \"F1 LR\": lr_score,\n",
    "                    \"F1 RF\": rf_score,\n",
    "                    \"n_features\": len(features_hdbscan.features)\n",
    "                }\n",
    "                tuning_results.append(r)\n",
    "                print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYTZq8GZmBem"
   },
   "source": [
    "Download/upload the results of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvlKh4FVhQ6l"
   },
   "outputs": [],
   "source": [
    "df_tuning_results = pd.DataFrame(tuning_results)\n",
    "df_tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ouefc3K1xi-"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import datetime\n",
    "\n",
    "# Save the scores data as a CSV\n",
    "filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - HDBSCAN Hyperparameter Optimization.csv\"\n",
    "df_tuning_results.to_csv(filename, index=False)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VonexOk42ZG3"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import datetime\n",
    "\n",
    "# Upload the scores CSV\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the scores\n",
    "df_tuning_results = pd.read_csv(filename)\n",
    "df_tuning_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0T3CKWEmLDM"
   },
   "source": [
    "Plot the results of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Wwp2SJen6uK"
   },
   "outputs": [],
   "source": [
    "# Simplify the dataset IDs for visualization\n",
    "df_tuning_results[\"Dataset\"] = df_tuning_results[\"Dataset\"].map({\n",
    "    \"cardiffnlp/tweet_sentiment_multilingual\": \"Sentiment\",\n",
    "    \"hate_speech18\": \"Hate Speech\",\n",
    "    \"amazon_polarity\": \"Amazon\",\n",
    "    \"GonzaloA/fake_news\": \"Fake News\",\n",
    "    \"tum-nlp/IDMGSP\": \"Papers\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXXEsYVYo1EM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from google.colab import files\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Unpack the axis for easy reference\n",
    "(ax1, ax2) = axs\n",
    "\n",
    "# Plot boxplot for keep_noise\n",
    "sns.boxplot(x='Dataset', y='F1 Average', hue='keep_noise', data=df_tuning_results, ax=ax1)\n",
    "ax1.set_title(\"Performance for Different Values of 'keep_noise'\")\n",
    "ax1.set_ylabel(\"F1 Score\")\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylim([0.3, 1.0])\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# Plotting boxplot for cluster_selection_method\n",
    "sns.boxplot(x='Dataset', y='F1 Average', hue='cluster_selection_method', data=df_tuning_results, ax=ax2)\n",
    "ax2.set_title(\"Performance for Different Values of 'cluster_selection_method'\")\n",
    "ax2.set_ylabel(\"F1 Score\")\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.set_ylim([0.3, 1.0])\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# Add an overall title\n",
    "fig.suptitle(\"HDBSCAN: Hyperparameter Tuning (Categorical Features)\")\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Download the plot as PDF\n",
    "filename = \"HDBSCAN Categorical Hyperparameter Optimization A.pdf\"\n",
    "plt.savefig(filename)\n",
    "files.download(filename)\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCz-HCJp-n8b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Filter the dataframe for the optimal values of other hyperparameters\n",
    "df_filtered = df_tuning_results[\n",
    "    (df_tuning_results['keep_noise'] == False) &\n",
    "    (df_tuning_results['cluster_selection_method'] == 'leaf')\n",
    "]\n",
    "\n",
    "# Get unique datasets\n",
    "unique_datasets = df_filtered['Dataset'].unique()\n",
    "\n",
    "# Prepare contour plots data with the filtered dataframe\n",
    "contour_plots_data = []\n",
    "for dataset in unique_datasets:\n",
    "    dataset_df = df_filtered[df_filtered['Dataset'] == dataset]\n",
    "    dataset_grouped = dataset_df.groupby(['alpha', 'epsilon'])['F1 Average'].mean().reset_index()\n",
    "    pivot_table = dataset_grouped.pivot(index='alpha', columns='epsilon', values='F1 Average')\n",
    "    contour_plots_data.append((dataset, pivot_table))\n",
    "\n",
    "# Calculate the average F1 score across all datasets\n",
    "average_f1 = df_filtered.groupby(['alpha', 'epsilon'])['F1 Average'].mean().reset_index()\n",
    "average_f1_pivot = average_f1.pivot(index='alpha', columns='epsilon', values='F1 Average')\n",
    "contour_plots_data.append(('Average across Datasets', average_f1_pivot))\n",
    "\n",
    "# Determine global min and max F1 scores for consistent color scaling across all plots\n",
    "f1_min = min(data.min().min() for _, data in contour_plots_data)\n",
    "f1_max = 1.0\n",
    "\n",
    "# Create a new figure for the plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each dataset with the unified color scaling\n",
    "for i, (dataset, data) in enumerate(contour_plots_data):\n",
    "    sns.heatmap(data, ax=axes[i], cmap=\"YlGnBu_r\", cbar=False, linewidths=0, vmin=f1_min, vmax=f1_max)\n",
    "    axes[i].invert_yaxis()\n",
    "    axes[i].set_title(f'{dataset}')\n",
    "    axes[i].set_ylabel('Alpha' if i % 3 == 0 else '')   # Only show the y axis label once for each row\n",
    "    axes[i].set_xlabel('Epsilon' if i >= 3 else '')     # Only show the x axis label once for each column\n",
    "    axes[i].set_yticks(axes[i].get_yticks(), axes[i].get_yticklabels(), rotation=0)\n",
    "\n",
    "# Place a color bar at the right of the plots\n",
    "cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(axes[0].collections[0], cax=cbar_ax)\n",
    "cbar.set_label('F1 Score')\n",
    "cbar.set_ticks(np.linspace(f1_min, f1_max, num=5))\n",
    "cbar.ax.set_yticklabels([f'{tick:.0%}' for tick in cbar.get_ticks()])\n",
    "\n",
    "# Add an overall title\n",
    "fig.suptitle(\"HDBSCAN: Hyperparameter Tuning (Numerical Features)\")\n",
    "\n",
    "# Adjust layout for better fit and to make room for the color bar\n",
    "fig.subplots_adjust(right=0.85)\n",
    "\n",
    "# Download the plot as PDF\n",
    "filename = \"HDBSCAN Numerical Hyperparameter Optimization B.pdf\"\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "files.download(filename)\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyV9zhjOssfL"
   },
   "outputs": [],
   "source": [
    "clusters_hdbscan = felix._cluster_features(felix._feature_embeddings)\n",
    "features_hdbscan = felix._select_representative_features(felix._features, felix._feature_embeddings, clusters_hdbscan)\n",
    "print(f\"HDBSCAN selected {len(features_hdbscan.features)} features\")\n",
    "\n",
    "lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_hdbscan.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n",
    "score_hdbscan = np.mean([lr_score, rf_score])\n",
    "print(f\"Score = {score_hdbscan}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAORS-LFoCyN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk66JXllrOme"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scores_kmeans = []\n",
    "\n",
    "for n_clusters in range(1, len(df_scores.columns)+1):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=\"auto\")\n",
    "    kmeans.fit(felix._feature_embeddings)\n",
    "    clusters_kmeans = kmeans.labels_\n",
    "    features_kmeans = felix._select_representative_features(felix._features, felix._feature_embeddings, clusters_kmeans)\n",
    "\n",
    "    lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_kmeans.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n",
    "    score = np.mean([lr_score, rf_score])\n",
    "    scores_kmeans.append(score)\n",
    "\n",
    "    log_results(d.id, \"K-means\", felix, [f.name for f in features_kmeans.features], rf_score, lr_score, lr_prob)\n",
    "\n",
    "    print(f\"n_features = {n_clusters}. Score = {score}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahsiWVgUoFnA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYr5ZpbXrOji"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Fit agglomerative hierarchical clustering\n",
    "agg_clustering = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "agg_clustering.fit(felix._feature_embeddings)\n",
    "\n",
    "# Create a linkage matrix from the AgglomerativeClustering output\n",
    "num_samples = len(agg_clustering.labels_)\n",
    "linkage_matrix = np.column_stack([\n",
    "    agg_clustering.children_,\n",
    "    agg_clustering.distances_,\n",
    "    [num_samples + i for i in range(num_samples - 1)]\n",
    "])\n",
    "\n",
    "scores_agg = []\n",
    "\n",
    "for n_clusters in range(1, len(df_scores.columns)+1):\n",
    "    # Use fcluster to get the cluster labels based on a threshold for number of clusters\n",
    "    threshold = linkage_matrix[:, 2][-(n_clusters - 2)]\n",
    "    clusters_agg = fcluster(linkage_matrix, t=threshold, criterion=\"distance\")\n",
    "\n",
    "    features_agg = felix._select_representative_features(felix._features, felix._feature_embeddings, clusters_agg)\n",
    "\n",
    "    lr_prob, lr_score, rf_score = evaluate_feature_set([f.name for f in features_agg.features], df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n",
    "    score = np.mean([lr_score, rf_score])\n",
    "    scores_agg.append(score)\n",
    "\n",
    "    log_results(d.id, \"Agglomerative Clustering\", felix, [f.name for f in features_agg.features], rf_score, lr_score, lr_prob)\n",
    "\n",
    "    print(f\"n_features = {n_clusters}. Score = {score}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IREyoPaBoOTn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Random Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhaqU4azkAsk"
   },
   "source": [
    "Run random feature selection 5 times for each dataset and average the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92bQY-e5VT13"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "    scores_random = []\n",
    "\n",
    "    # Create a random pertubation of the feature set\n",
    "    features_shuffled = felix._features.copy(deep=True)\n",
    "    random.shuffle(features_shuffled.features)\n",
    "\n",
    "    for n_features in range(1, len(df_scores.columns)+1):\n",
    "        feature_set = features_shuffled.copy(deep=True)\n",
    "        feature_set.features = features_shuffled.features[:n_features]\n",
    "        feature_names = [f.name for f in features_shuffled.features[:n_features]]\n",
    "\n",
    "        lr_prob, lr_score, rf_score = evaluate_feature_set(feature_names, df_scores, d.y_test, cv_splits=5, random_state=42, lr_probs=True, lr_f1=True, rf_f1=True)\n",
    "        score = np.mean([lr_score, rf_score])\n",
    "        scores_random.append(score)\n",
    "\n",
    "        log_results(d.id, \"Random Feature Selection\", felix, feature_names, rf_score, lr_score, lr_prob)\n",
    "\n",
    "        print(f\"iteration = {i + 1}. n_features = {n_features}. Score = {score}. LR_prob = {lr_prob}. LR_F1 = {lr_score}. RF_F1 = {rf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us9M2NwOPhri",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEfcN_CWPldl"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "json_string = json.dumps(results_log, indent=4)\n",
    "\n",
    "# Store the results as a JSON\n",
    "filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Consolidation Results {d.short_name}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(json_string)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)\n",
    "\n",
    "\n",
    "# Convert the results log to a Pandas DataFrame\n",
    "df_results = pd.DataFrame(results_log)\n",
    "\n",
    "# Save the results as a CSV\n",
    "filename = f\"{datetime.date.today().strftime('%Y_%m_%d')} - FELIX Consolidation Results {d.short_name}.csv\"\n",
    "df_results.to_csv(filename, index=False)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvOZtzqKRINh"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload the results JSON\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the scores\n",
    "results_log = json.loads(uploaded[filename])\n",
    "len(results_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTPGzuqyoSj5",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crXgaQzFycXJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert the results log to a Pandas DataFrame\n",
    "df_results = pd.DataFrame(results_log)\n",
    "\n",
    "# Calcualte average F1 scores from Random Forest and Logistic Regression\n",
    "df_results[\"F1 Score\"] = df_results.apply(lambda row: np.mean(row[[\"f1_rf\", \"f1_lr\"]]), axis=1)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH9DrtJUyfha"
   },
   "outputs": [],
   "source": [
    "df_results.groupby([\"dataset\", \"method\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PZaOjbGHx-8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert the results log to a Pandas DataFrame\n",
    "df_results = pd.DataFrame(results_log)\n",
    "\n",
    "# Calcualte average F1 scores from Random Forest and Logistic Regression\n",
    "df_results[\"F1 Score\"] = df_results.apply(lambda row: np.mean(row[[\"f1_rf\", \"f1_lr\"]]), axis=1)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJl6awx3UteY"
   },
   "outputs": [],
   "source": [
    "df_results.groupby([\"dataset\", \"method\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-kB_R10mXnZ"
   },
   "outputs": [],
   "source": [
    "# Simplify the dataset IDs for visualization\n",
    "df_results[\"dataset\"] = df_results[\"dataset\"].map({\n",
    "    \"cardiffnlp/tweet_sentiment_multilingual\": \"Sentiment\",\n",
    "    \"hate_speech18\": \"Hate Speech\",\n",
    "    \"amazon_polarity\": \"Amazon\",\n",
    "    \"GonzaloA/fake_news\": \"Fake News\",\n",
    "    \"tum-nlp/IDMGSP\": \"Papers\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFpb8BRDtjhS"
   },
   "outputs": [],
   "source": [
    "# Set the order for the datasets\n",
    "datasets_order = [\n",
    "    \"Sentiment\",\n",
    "    \"Hate Speech\",\n",
    "    \"Amazon\",\n",
    "    \"Fake News\",\n",
    "    \"Papers\"\n",
    "]\n",
    "\n",
    "# Set the order for the legend\n",
    "methods_order = [\n",
    "    \"Random Feature Selection\",\n",
    "    \"Supervised Forward Selection\",\n",
    "    \"Agglomerative Clustering\",\n",
    "    \"K-means\",\n",
    "    \"HDBSCAN\"\n",
    "]\n",
    "\n",
    "# Define a color for each method\n",
    "colors = {\n",
    "    \"Supervised Forward Selection\": \"#969696\",\n",
    "    \"HDBSCAN\": \"#e6550d\",\n",
    "    \"Agglomerative Clustering\": \"#6baed6\",\n",
    "    \"K-means\": \"#74c476\",\n",
    "    \"Random Feature Selection\": \"#d9d9d9\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pxuqd9-IY0D9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Group by 'n_features' and 'method' to calculate the mean of 'F1 Score' for duplicated entries (e.g., for random feature selection which has been executed multiple times)\n",
    "df_adjusted = df_results.groupby(['n_features', 'method', 'dataset'])['F1 Score'].mean().reset_index()\n",
    "\n",
    "# Drop entries for HDBSCAN with noise\n",
    "df_adjusted = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN (w/ noise)\"]\n",
    "df_adjusted[\"method\"] = df_adjusted[\"method\"].replace(\"HDBSCAN (w/o noise)\", \"HDBSCAN\")\n",
    "df_adjusted = df_adjusted.reset_index()\n",
    "\n",
    "# Create a grid of 2x3 for the scatter plots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot a scatter plot for each dataset\n",
    "for i, dataset in enumerate(datasets_order):\n",
    "    df_subset = df_adjusted[df_adjusted['dataset'] == dataset]\n",
    "    for method in methods_order:\n",
    "        df_method = df_subset[df_subset['method'] == method]\n",
    "        axes[i].scatter(df_method['n_features'], df_method['F1 Score'], label=method, s=(15 if method == \"HDBSCAN\" else 3), color=colors[method], zorder=3)\n",
    "    axes[i].set_title(dataset)\n",
    "    axes[i].set_ylabel('F1 Score' if i % 2 == 0 else '')\n",
    "    axes[i].set_xlabel('Number of Selected Features' if i > 3 else '')\n",
    "    axes[i].set_xlim([0, max(df_subset['n_features'])])\n",
    "    axes[i].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "    axes[i].grid(True, color='lightgrey', zorder=0)\n",
    "\n",
    "# Plot a scotter plot of the average across all datasets\n",
    "df_global_average = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN\"].groupby(['n_features', 'method']).mean(numeric_only=True).reset_index()\n",
    "hdbscan_average = df_adjusted[df_adjusted[\"method\"] == \"HDBSCAN\"][[\"n_features\", \"F1 Score\"]].mean()\n",
    "least_features = df_adjusted[[\"dataset\", \"n_features\"]].groupby(\"dataset\").max()[\"n_features\"].min()\n",
    "for method in methods_order:\n",
    "    df_method = df_global_average[df_global_average['method'] == method]\n",
    "    axes[5].scatter(df_method['n_features'], df_method['F1 Score'], label=method, s=3, color=colors[method], zorder=3)\n",
    "axes[5].scatter(hdbscan_average[\"n_features\"], hdbscan_average[\"F1 Score\"], label=\"HDBSCAN\", s=15, color=colors[\"HDBSCAN\"], zorder=3)\n",
    "axes[5].set_title(\"Average across Datasets\")\n",
    "axes[5].set_xlim([0, least_features])\n",
    "axes[5].set_xlabel('Number of Selected Features')\n",
    "axes[5].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "axes[5].grid(True, color='lightgrey', zorder=0)\n",
    "\n",
    "# Create a single legend for all plots\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=len(methods_order), bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "# Add a chart title\n",
    "fig.suptitle('Feature Consolidation Performance (Categorical Features)', y=1.05)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Download the plot as PDF\n",
    "filename = \"Feature Consolidation Methods Categorical (All Datasets).pdf\"\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "files.download(filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oj6cUsoqHtN6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Group by 'n_features' and 'method' to calculate the mean of 'F1 Score' for duplicated entries (e.g., for random feature selection which has been executed multiple times)\n",
    "df_adjusted = df_results.groupby(['n_features', 'method', 'dataset'])['F1 Score'].mean().reset_index()\n",
    "\n",
    "# Drop entries for HDBSCAN with noise\n",
    "df_adjusted = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN (w/ noise)\"]\n",
    "df_adjusted[\"method\"] = df_adjusted[\"method\"].replace(\"HDBSCAN (w/o noise)\", \"HDBSCAN\")\n",
    "df_adjusted = df_adjusted.reset_index()\n",
    "\n",
    "# Calculate average results across all datasets\n",
    "df_global_average = df_adjusted[df_adjusted[\"method\"] != \"HDBSCAN\"].groupby(['n_features', 'method']).mean(numeric_only=True).reset_index()\n",
    "least_features = df_adjusted[[\"dataset\", \"n_features\"]].groupby(\"dataset\").max()[\"n_features\"].min()\n",
    "\n",
    "# Create a plot\n",
    "fig = plt.figure(figsize=(6, 3.5))\n",
    "\n",
    "# Add results for each consolidation method\n",
    "for method in methods_order:\n",
    "    if method == \"HDBSCAN\":\n",
    "        hdbscan_average = df_adjusted[df_adjusted[\"method\"] == \"HDBSCAN\"][[\"n_features\", \"F1 Score\"]].mean()\n",
    "        plt.scatter(hdbscan_average[\"n_features\"], hdbscan_average[\"F1 Score\"], label=\"HDBSCAN\", s=25, color=colors[\"HDBSCAN\"], zorder=3)\n",
    "    else:\n",
    "        df_method = df_global_average[df_global_average['method'] == method]\n",
    "        plt.scatter(df_method['n_features'], df_method['F1 Score'], label=method, s=3, color=colors[method], zorder=3)\n",
    "\n",
    "# Format the plot\n",
    "plt.title(\"Feature Consolidation Performance (Categorical Features)\")\n",
    "plt.xlim([0, least_features + 0.5])\n",
    "plt.ylim([0.57, 0.87])\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "plt.grid(True, color='lightgrey', zorder=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Download the plot as PDF\n",
    "filename = \"Feature Consolidation Methods Categorical (Average).pdf\"\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "files.download(filename)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlaI6pvAURpf"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "df_feature_selection = pd.DataFrame({\"Forward Selection\": scores_forward, \"K-means\": scores_kmeans, \"Agglomerative\": scores_agg, \"Random\": scores_random})\n",
    "\n",
    "filename = \"2023_11_02 - FELIX Feature Selection F1 Scores Fake News.csv\"\n",
    "df_feature_selection.to_csv(filename, index=False)\n",
    "\n",
    "# Download the results\n",
    "files.download(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhStKTtc8qwH"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the results\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the scores\n",
    "df_feature_selection = pd.read_csv(filename)\n",
    "\n",
    "scores_forward = df_feature_selection[\"Forward Selection\"].tolist()\n",
    "scores_kmeans = df_feature_selection[\"K-means\"].tolist()\n",
    "scores_agg = df_feature_selection[\"Agglomerative\"].tolist()\n",
    "scores_random = df_feature_selection[\"Random\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_fPiP34H0zV"
   },
   "source": [
    "## Experiment 5.2: Scoring Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ln-30kIZE3N"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def gini_index(categories):\n",
    "    # Count the occurrences of each category\n",
    "    category_counts = Counter(categories)\n",
    "    # Calculate the proportion of each category and square the proportions\n",
    "    squared_proportions = [(count / len(categories)) ** 2 for count in category_counts.values()]\n",
    "    # Sum the squared proportions and subtract from 1\n",
    "    gini = 1 - sum(squared_proportions)\n",
    "    return gini\n",
    "\n",
    "# Given array\n",
    "categories = [\"A\", \"A\", \"C\", \"A\", \"B\", \"C\"]\n",
    "categories = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "\n",
    "# Calculate Gini index\n",
    "gini = gini_index(categories)\n",
    "\n",
    "print(f\"Gini index of the array: {gini}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD4qeV3AgmGR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setup: Learn Numeric Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkgdBkrBOrRV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxA2lp3jeR6T"
   },
   "outputs": [],
   "source": [
    "# Select a dataset\n",
    "d = dataset_list[\"papers\"]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC_acG8Bd6Hc"
   },
   "outputs": [],
   "source": [
    "# Fit FELIX to the dataset to learn features\n",
    "felix = FELIX(\n",
    "    context=d.context,\n",
    "    temperature_scoring=0.0,\n",
    "    discrete_features=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "felix.fit(d.X_train, d.y_train)\n",
    "\n",
    "# Store the features learned by FELIX\n",
    "feature_set = felix._features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHUEert8a5x0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Case: Stable Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqtZshJUbEGr"
   },
   "outputs": [],
   "source": [
    "# Get a random example\n",
    "sample_row = d.X_test.sample(1, random_state=seed)\n",
    "\n",
    "# Replicate the example 10 times and create a new dataframe\n",
    "df = pd.concat([sample_row]*10, ignore_index=True)\n",
    "\n",
    "# Score the example using FELIX\n",
    "print(\"Scoring the same example 10 times ...\")\n",
    "df_scores_stable = felix.transform(df)\n",
    "\n",
    "# Score different examples using FELIX\n",
    "print(\"Scoring 10 different examples ...\")\n",
    "df_sample = d.X_test.sample(10, random_state=seed)\n",
    "df_scores_general = felix.transform(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkXuwhCElPOk"
   },
   "outputs": [],
   "source": [
    "df_variance = pd.DataFrame({\"Inner-sample\": df_scores_stable.var(), \"Inter-sample\": df_scores_general.var()})\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "ax = df_variance.boxplot(vert=False)\n",
    "ax.set_xlabel(\"Score variance\")\n",
    "ax.set_title(\"Distribution of score variance inner-sample vs. inter-sample\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_variance.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3rwq4_wmk9M"
   },
   "source": [
    "Alternative plot showing the score distribution per feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxi6mHIRhvgf"
   },
   "outputs": [],
   "source": [
    "# Create a boxplot for the value distribution of each feature\n",
    "plt.figure(figsize=(8, df_scores_stable.shape[1] / 5))\n",
    "ax = df_scores_stable.boxplot(vert=False)\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title('Score distribution (stable order)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkfJ6_FZacN6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Case: Random Reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C4ztCrKafRL"
   },
   "outputs": [],
   "source": [
    "# Get a random example\n",
    "sample_row = d.X_test.sample(1, random_state=seed)\n",
    "\n",
    "# Keep the scoring LLM stable with a 16K context window\n",
    "felix.llm_scoring = \"gpt-3.5-turbo-16k\"\n",
    "felix.verbose = True\n",
    "\n",
    "# Score the original feature set 10 times\n",
    "print(\"Scoring the original feature set ...\")\n",
    "felix._features = feature_set\n",
    "results_rows_original = []\n",
    "for _ in tqdm(range(10)):\n",
    "    results_rows_original.append(felix.transform(sample_row))\n",
    "df_scores_original = pd.concat(results_rows_original).reset_index(drop=True)\n",
    "\n",
    "# Score a randomly reordered feature set 10 times\n",
    "print(\"Scoring the randomly reordered feature set ...\")\n",
    "random.shuffle(felix._features.features) # Randomly reorders the feature set in place\n",
    "results_rows_random = []\n",
    "for _ in tqdm(range(10)):\n",
    "    results_rows_random.append(felix.transform(sample_row))\n",
    "df_scores_random = pd.concat(results_rows_random).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJctgpp0gAHJ"
   },
   "outputs": [],
   "source": [
    "# Perform t-test to see if changing the scoring LLM results in significantly different scores\n",
    "def test_significance(df_model_a, df_model_b):\n",
    "    p_values = {}\n",
    "    for column in df_model_a.columns:\n",
    "        _, p_value = stats.ttest_rel(df_model_a[column], df_model_b[column])\n",
    "        p_values[column] = p_value\n",
    "    return pd.Series(p_values)\n",
    "\n",
    "p_values = test_significance(df_scores_original, df_scores_random)\n",
    "\n",
    "# Excluding NaN values for plotting\n",
    "p_values_cleaned = p_values.dropna()\n",
    "\n",
    "# Plotting boxplots\n",
    "plt.figure(figsize=(7, 1.5))\n",
    "plt.boxplot([p_values_cleaned], vert=False, labels=[\"Random reordering\"])\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.title(\"t-test for score differences when randomly reordering the feature set\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Computing statistics\n",
    "stats_random_reordering = {\n",
    "    \"Avg. p-value\": [p_values_cleaned.mean()],\n",
    "    \"% of p-values < 0.01\": [(p_values_cleaned < 0.01).mean() * 100],\n",
    "    \"% of p-values < 0.05\": [(p_values_cleaned < 0.05).mean() * 100],\n",
    "    \"% of p-values < 0.1\": [(p_values_cleaned < 0.1).mean() * 100]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df_stats_random_reordering = pd.DataFrame(stats_random_reordering, index=[\"Random reordering\"])\n",
    "df_stats_random_reordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHblwbnRfyHi"
   },
   "source": [
    "Alternative experiment showing the large variance in scores when the feature set is randomly reschuffled in each scoring request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOdWDH23juEC"
   },
   "outputs": [],
   "source": [
    "# Get a random example\n",
    "sample_row = d.X_test.sample(1, random_state=seed)\n",
    "\n",
    "# List to store the transformed rows\n",
    "result_rows = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Randomly reorder the features\n",
    "    random.shuffle(felix._features.features)\n",
    "    print(f\"Iteration {_}: first feature is {felix._features.features[0].name}\")\n",
    "\n",
    "    # Transform the sample row and append the result to the list\n",
    "    transformed_row = felix.transform(sample_row)\n",
    "    result_rows.append(transformed_row)\n",
    "\n",
    "# Concatenate all resulting rows into a single dataframe\n",
    "df_scores_random = pd.concat(result_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWHIfaumneGi"
   },
   "outputs": [],
   "source": [
    "# Create a boxplot for the value distribution of each feature\n",
    "plt.figure(figsize=(8, df_scores_random.shape[1] / 5))\n",
    "ax = df_scores_random.boxplot(vert=False)\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title('Score distribution (random order)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiD9UCwjGe28",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Case: Splitting scoring requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf9YlmIME8oq"
   },
   "outputs": [],
   "source": [
    "# Get a random example\n",
    "sample_row = d.X_test.sample(1, random_state=seed)\n",
    "\n",
    "# Set scoring LLM to the 16K context variant of GPT-3.5 so that all features fit into the context of one request\n",
    "felix.llm_scoring=\"gpt-3.5-turbo-16k\"\n",
    "felix.verbose=False\n",
    "\n",
    "# Score the example 10 times with all features in one request\n",
    "print(\"Scoring with all features in one request ...\")\n",
    "felix._features = feature_set\n",
    "result_rows_no_split = []\n",
    "for _ in tqdm(range(10)):\n",
    "    # Transform the sample row and append the result to the list\n",
    "    result_rows_no_split.append(felix.transform(sample_row))\n",
    "df_scores_no_split = pd.concat(result_rows_no_split).reset_index(drop=True)\n",
    "\n",
    "# Score the example 10 times with only the first half of the feature set\n",
    "print(\"Scoring wih first half of the feature set ...\")\n",
    "felix._features = NumericalFeatureSet(features=feature_set.features[:len(feature_set.features)//2])\n",
    "result_rows_split_a = []\n",
    "for _ in tqdm(range(10)):\n",
    "    # Transform the sample row and append the result to the list\n",
    "    result_rows_split_a.append(felix.transform(sample_row))\n",
    "df_scores_split_a = pd.concat(result_rows_split_a).reset_index(drop=True)\n",
    "\n",
    "# Score the example 10 times with only the second half of the feature set\n",
    "print(\"Scoring wih second half of the feature set ...\")\n",
    "felix._features = NumericalFeatureSet(features=feature_set.features[len(feature_set.features)//2:])\n",
    "result_rows_split_b = []\n",
    "for _ in tqdm(range(10)):\n",
    "    # Transform the sample row and append the result to the list\n",
    "    result_rows_split_b.append(felix.transform(sample_row))\n",
    "df_scores_split_b = pd.concat(result_rows_split_b).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3KEn8qsSI8g"
   },
   "outputs": [],
   "source": [
    "# Perform t-test to see if splitting up scoring into multiple requests results in significantly different scores\n",
    "def test_significance(df_split, df_no_split):\n",
    "    p_values = {}\n",
    "    for column in df_split.columns:\n",
    "        if column in df_no_split.columns:\n",
    "            _, p_value = stats.ttest_rel(df_split[column], df_no_split[column])\n",
    "            p_values[column] = p_value\n",
    "    return pd.Series(p_values)\n",
    "\n",
    "p_values_split_a = test_significance(df_scores_split_a, df_scores_no_split)\n",
    "p_values_split_b = test_significance(df_scores_split_b, df_scores_no_split)\n",
    "\n",
    "# Excluding NaN values for plotting\n",
    "p_values_a_cleaned = p_values_split_a.dropna()\n",
    "p_values_b_cleaned = p_values_split_b.dropna()\n",
    "\n",
    "# Plotting boxplots\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.boxplot([p_values_a_cleaned, p_values_b_cleaned], vert=False, labels=[\"First half\", \"Second half\"])\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.title(\"t-test for score differences in first and second half of feature set vs. full feature set\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Computing statistics\n",
    "stats_split = {\n",
    "    \"Avg. p-value\": [p_values_a_cleaned.mean(), p_values_b_cleaned.mean()],\n",
    "    \"% of p-values < 0.01\": [(p_values_a_cleaned < 0.01).mean() * 100, (p_values_b_cleaned < 0.01).mean() * 100],\n",
    "    \"% of p-values < 0.05\": [(p_values_a_cleaned < 0.05).mean() * 100, (p_values_b_cleaned < 0.05).mean() * 100],\n",
    "    \"% of p-values < 0.1\": [(p_values_a_cleaned < 0.1).mean() * 100, (p_values_b_cleaned < 0.1).mean() * 100]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df_stats_split = pd.DataFrame(stats_split, index=[\"First half\", \"Second half\"])\n",
    "df_stats_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmDaAXcbT0yg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Case: Changing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-IR848HNnKX"
   },
   "outputs": [],
   "source": [
    "# Get a random example\n",
    "sample_row = d.X_test.sample(1, random_state=seed)\n",
    "\n",
    "# Limit the number of features to score due to small context window\n",
    "max_features = 40\n",
    "felix._features = NumericalFeatureSet(features=feature_set.features[:min(max_features, len(feature_set.features))])\n",
    "felix.verbose=True\n",
    "\n",
    "# Score the example 10 times with 4K context window\n",
    "print(\"Scoring with with 4K context window ...\")\n",
    "felix.llm_scoring=\"gpt-3.5-turbo\"\n",
    "result_rows_4k = []\n",
    "for _ in tqdm(range(10)):\n",
    "    # Transform the sample row and append the result to the list\n",
    "    result_rows_4k.append(felix.transform(sample_row))\n",
    "df_scores_4k = pd.concat(result_rows_4k).reset_index(drop=True)\n",
    "\n",
    "# Score the example 10 times with 16K context window\n",
    "print(\"Scoring with with 16K context window ...\")\n",
    "felix.llm_scoring=\"gpt-3.5-turbo-16k\"\n",
    "result_rows_16k = []\n",
    "for _ in tqdm(range(10)):\n",
    "    # Transform the sample row and append the result to the list\n",
    "    result_rows_16k.append(felix.transform(sample_row))\n",
    "df_scores_16k = pd.concat(result_rows_16k).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOrMIdiwVQsJ"
   },
   "outputs": [],
   "source": [
    "# Perform t-test to see if changing the scoring LLM results in significantly different scores\n",
    "def test_significance(df_model_a, df_model_b):\n",
    "    p_values = {}\n",
    "    for column in df_model_a.columns:\n",
    "        _, p_value = stats.ttest_rel(df_model_a[column], df_model_b[column])\n",
    "        p_values[column] = p_value\n",
    "    return pd.Series(p_values)\n",
    "\n",
    "p_values = test_significance(df_scores_4k, df_scores_16k)\n",
    "\n",
    "# Excluding NaN values for plotting\n",
    "p_values_cleaned = p_values.dropna()\n",
    "\n",
    "# Plotting boxplots\n",
    "plt.figure(figsize=(7, 1.5))\n",
    "plt.boxplot([p_values_cleaned], vert=False, labels=[\"Model change\"])\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.title(\"t-test for score differences when changing the scoring LLM\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Computing statistics\n",
    "stats_model_change = {\n",
    "    \"Avg. p-value\": [p_values_cleaned.mean()],\n",
    "    \"% of p-values < 0.01\": [(p_values_cleaned < 0.01).mean() * 100],\n",
    "    \"% of p-values < 0.05\": [(p_values_cleaned < 0.05).mean() * 100],\n",
    "    \"% of p-values < 0.1\": [(p_values_cleaned < 0.1).mean() * 100]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df_stats_model_change = pd.DataFrame(stats_model_change, index=[\"Model change\"])\n",
    "df_stats_model_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcO0HGZofFsn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### E2E Performance when Reschuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u5KQ7mnfiBv"
   },
   "outputs": [],
   "source": [
    "d = dataset_list[\"papers\"]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YQrsqEQfJsE"
   },
   "outputs": [],
   "source": [
    "callback = CustomFELIXCallback()\n",
    "felix = FELIX(context=d.context_train, discrete_features=False, callback=callback, verbose=True)\n",
    "\n",
    "felix.fit(d.X_train, d.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wf7kExMgeBU"
   },
   "outputs": [],
   "source": [
    "original_features = felix._features\n",
    "len(original_features.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBq07sqtfb3Q"
   },
   "outputs": [],
   "source": [
    "felix.reschuffle_features = True\n",
    "\n",
    "# Transform data using feature reschuffling\n",
    "df_scores_train_reschuffle = felix.transform(d.X_train)\n",
    "df_scores_test_reschuffle = felix.transform(d.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTMNrbKk6JXK"
   },
   "outputs": [],
   "source": [
    "data_transformer = DataTransformer(dataset=d)\n",
    "df_scores_train_reschuffle_imputed, df_scores_test_reschuffle_imputed = data_transformer.impute_missing_values(df_scores_train_reschuffle, df_scores_test_reschuffle)\n",
    "\n",
    "columns = [col for col in df_scores_train_reschuffle_imputed.columns if col in df_scores_test_reschuffle_imputed.columns]\n",
    "\n",
    "df_scores_train_reschuffle_imputed = df_scores_train_reschuffle_imputed[columns]\n",
    "df_scores_test_reschuffle_imputed = df_scores_test_reschuffle_imputed[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4icf74q5oha"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(df_scores_train_reschuffle_imputed, d.y_train)\n",
    "y_pred_rf = rf.predict(df_scores_test_reschuffle_imputed)\n",
    "\n",
    "print(\"F1 Random Forest:\", f1_score(d.y_test, y_pred_rf, average=\"macro\"))\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(df_scores_train_reschuffle_imputed, d.y_train)\n",
    "y_pred_lr = lr.predict(df_scores_test_reschuffle_imputed)\n",
    "\n",
    "print(\"F1 Logistic Regression:\", f1_score(d.y_test, y_pred_lr, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KW1ajn5Jh8Po"
   },
   "outputs": [],
   "source": [
    "felix.reschuffle_features = False\n",
    "felix._features = original_features\n",
    "\n",
    "df_scores_train_stable = felix.transform(d.X_train)\n",
    "df_scores_test_stable = felix.transform(d.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCzH1yN8Dfrm"
   },
   "outputs": [],
   "source": [
    "data_transformer = DataTransformer(dataset=d)\n",
    "df_scores_train_stable_imputed, df_scores_test_stable_imputed = data_transformer.impute_missing_values(df_scores_train_stable, df_scores_test_stable)\n",
    "\n",
    "columns = [col for col in df_scores_train_stable_imputed.columns if col in df_scores_test_stable_imputed.columns]\n",
    "\n",
    "df_scores_train_stable_imputed = df_scores_train_stable_imputed[columns]\n",
    "df_scores_test_stable_imputed = df_scores_test_stable_imputed[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X8Iu-AYiGLp"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(df_scores_train_stable_imputed, d.y_train)\n",
    "y_pred_rf = rf.predict(df_scores_test_stable_imputed)\n",
    "\n",
    "print(\"F1 Random Forest:\", f1_score(d.y_test, y_pred_rf, average=\"macro\"))\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(df_scores_train_stable_imputed, d.y_train)\n",
    "y_pred_lr = lr.predict(df_scores_test_stable_imputed)\n",
    "\n",
    "print(\"F1 Logistic Regression:\", f1_score(d.y_test, y_pred_lr, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzSVy4wViMQZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOB+Hsq40fqb8uNwMTQLrrp",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
