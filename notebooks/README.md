
# Итоговый отчёт по проекту
## Цель проекта
Создать систему, которая на основе резюме кандидатов сможет предсказать, пройдёт ли кандидат первый этап отбора. Изначально предполагалось использовать описание вакансий, но позднее задача была упрощена до предсказания прохождения первого этапа на основании только резюме.
________________________________________
# Этапы выполнения проекта
## 1. Первый подход: использование LLM для анализа резюме (assesment_LLM/)
В рамках первого подхода применялись большие языковые модели (LLM), чтобы оценивать кандидатов и предсказывать их прохождение.
1.	Методология
-	Использовалась средняя локальная модель от X5.
-	Были созданы различные промпты, включающие ключевые метрики и признаки для анализа.
-	Промпты уточнялись, чтобы уменьшить вероятность галлюцинаций модели и повысить стабильность ответов.
2.	Результаты
-	F1-score составил ~0.5, ROC-AUC – ~0.53.
-	Результаты оказались непредсказуемыми: модель давала разные ответы на одинаковые данные в зависимости от промпта.
-	Итог: метод не смог адекватно ранжировать кандидатов.
3.	Выводы
-	Ограничения LLM (недостаток мощности локальной модели, проблемы с данными или их структурой) привели к слабым результатам.
-	Было принято решение использовать другой подход.
________________________________________
## 2. Второй подход: обучение модели E5-Large (embeddings/)
Для решения задачи мы попробовали использовать модель Multilingual E5-Large, чтобы извлечь эмбеддинги и обучить классификатор на их основе.
1.	Этапы работы
-	Извлечение эмбеддингов:
Эмбеддинги из резюме извлекались в файле embed.ipynb.
-	Обучение LoRA-адаптера:
    - Реализовано в файле train_model.ipynb.
    - Адаптер обучался на метках (0 – не проходит, 1 – проходит).
    - Перебирались параметры LoRA (ранг, ядра, скорость обучения и др).
-	Оценка адаптера:
Проводилась в файле eval_model.ipynb.
-	Обучение SVM:
    - Реализовано в файле SVC_optuna.ipynb.
    - Для подбора гиперпараметров использовалась optuna.
2.	Результаты
-	LoRA-адаптер:
    - На тестовых данных F1-score не превышал 0.51, несмотря на хорошие результаты на обучающей и валидационной выборках.
    - Ручной перебор параметров и подборка seed не улучшила результаты.
-	SVM:
    - F1-score на тестовых данных: ~0.48–0.52.
    - Перебор параметров через Optuna (1000+ экспериментов) не улучшил результаты.
3.	Выводы
-	Эмбеддинги E5-Large не смогли отразить структуру данных.
-	Методы LoRA и SVC оказались неэффективными для данной задачи.
________________________________________
## 3. Анализ данных и попытка улучшения с помощью визуализации (embeddings/)
1.	Гипотеза:
-	Предположили, что бинарное деление на классы higher и no higher слишком примитивно.
-	Данные были разделены на 5 классов:
    - Strong no higher, No higher, Weak higher, Higher, Strong higher.
2.	Визуализация:
-	Реализовано в файлах interns.ipynb.
-	Использовались эмбеддинги E5 и методы снижения размерности (t-SNE) с разными метриками расстояний (косинусное, Чебышёва, Евклидово и др.).
3.	Результаты:
-	Данные оказались неразделимыми, даже при делении на 5 классов.
-	Корреляции между признаками и целевой меткой были крайне низкими (≤0.2). 
   
4.	Вывод:
-	Гипотеза об одиночной межклассовой корреляции и наличии скрытых структур в данных не подтвердилась.
________________________________________
## 4. Перебор текстовых эмбеддингов (embeddings/)
После неудач с E5-Large был проведён систематический перебор других методов генерации текстовых эмбеддингов для анализа данных.
1.	Используемые методы:
-	Qwen Embeddings:
    - Реализация в interns_qwen.ipynb. Результаты оказались схожими с E5 – данные неразделимы.
-	Классические методы:
    - Реализация в interns_vecorizers.ipynb.
    - TF-IDF, Bag of Words, Ngram, Character, Hashing, Word2Vec, FastText.
    - Визуализация через t-SNE в 2D и 3D, как для 5, так и для 2 классов.
-	Все методы были визуализированы с использованием разных метрик расстояний
2.	Результаты:
-	Независимо от метода эмбеддингов, данные оставались неразделимыми.
-	Границы между классами не выявлялись даже визуально.
   
3.	Вывод:
-	Методы классических эмбеддингов и более современных моделей (E5, Qwen) оказались неспособны выделить значимые структуры в данных.
________________________________________
## 5. Извлечение интерпретируемых фичей (features_extracting_LLM/, features_extracting_re/)
1.	Методы:
-	LLM (файлы structured-output.py):
    - Использовалась средняя модель X5 для извлечения признаков в структурированном формате.
    - Результаты анализировались в json_prepare.ipynb и json_prepare_viz.ipynb.
-	Регулярные выражения (файлы simple-features.py):
    - С помощью регулярных выражений извлекались различные признаки (длина текста, источники, упоминания университетов, ключевые слова).
    - Анализировались в json_prepare_simple.ipynb и json_prepare_viz_simple.ipynb.
2.	Результаты:
-	Корреляции между фичами и меткой не превышали 0.2.
-	Модели на основе этих фичей давали F1-score ~0.54, Roc-Auc не превышал 0.54
-	SVM для обоих методов достигал F1 ~0.65, но классифицировал всё как один класс.
3.	Выводы:
-	Использование LLM и регулярок не позволяют извлечь достаточно сложные признаки (релевантный стаж, ВУЗ, высшая ступень образования, общая структура резюме). Как следствие, по имеющимся признакам затруднительно обучить модель, которая с достаточной точностью способна была бы предсказывать вероятность прохождения технического собеседования кандидатом. 
________________________________________
## 6. Попытка использования метода FELIX (FELIX/)
1.	Описание метода:
-	FELIX использует LLM для создания интерпретируемых категориальных и числовых фичей.
-	Результаты автора показывали, что классические модели (например, случайный лес) на таких данных могут превосходить fine-tuning LLM.
2.	Эксперименты:
-	Реализовано в файле felix_new.ipynb.
-	Для экспериментов использовалась локально развернутая модель Qwen-14B через ollama (sota в своем классе, превосходящая многие 32B-модели), поскольку нужно было проверить состоятельность нового метода на небольших моделях.
-	В экспериментах использовались открытые данные, предложенные автором.
-	OpenAI embedder был заменен на multilingual e5 large.
3.	Результаты:
-	FELIX работал корректно только с моделью GPT-4.
-	Модели меньшего размера, такие как Qwen-14B, GPT-3.5 и не показали результатов, сопоставимых с заявленными автором.
-	Генерируемые фичи были хуже, чем эмбеддинги E5.
   
4.	Вывод:
-	Метод не подходит для небольших локальных моделей, включая модели X5, из-за низкой производительности.
________________________________________
# Общие выводы по проекту
1.	Ключевая проблема:
-	Данные оказались слабо коррелируемыми с целевой меткой.
-	Даже сложные методы (LLM, embeddings, FELIX) не смогли выделить признаки, способные эффективно предсказывать результат.
2.	Попытки улучшения:
-	Визуализация данных подтвердила их слабую структурированность.
-	Интерпретируемые фичи не дали значительного прироста к метрикам.
3.	Рекомендации:
-	Требуется пересмотр данных: увеличение объёма, привлечение дополнительной информации.
-	Альтернативный подход – использование более мощных моделей (например, GPT-4) для метода FELIX в связке с улучшенными обезличенными данными.
Файлы, использованные на всех этапах, документируют эксперименты и подтверждают результаты.

